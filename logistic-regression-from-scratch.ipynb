{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fareselmenshawii/logistic-regression-from-scratch?scriptVersionId=144686736\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"padding:10px; \n            color:black;\n            margin:10px;\n            font-size:150%;\n            display:block;\n            border-radius:1px;\n            border-style: solid;\n            border-color:skyblue;\n            background-color:#00EFFF;\n            overflow:hidden;\">\n    <center>\n        <a id='top'></a>\n        <b style=\"color:black\">Table of Contents</b>\n    </center>\n    <ul>\n        <li>\n            <a href=\"#1\" style=\"color:black\">1 -  Overview</a>\n        </li>\n        <li>\n            <a href=\"#2\" style=\"color:black\">2 -  Imports</a>\n        </li>\n        <li>\n            <a href=\"#3\" style=\"color:black\">3 - Data Loading and Analysis</a>\n        </li>\n         <li>\n            <a href=\"#4\" style=\"color:black\">4 - Data Preprocessing</a>\n        </li>\n            <li>\n            <a href=\"#5\" style=\"color:black\">5 - Model Implementation </a>\n        </li>\n        <li>\n            <a href=\"#6\" style=\"color:black\">6 - Evaluation</a>\n        </li>\n        <li>\n            <a href=\"#7\" style=\"color:black\">7 - Thank you</a>\n        </li>\n    </ul>\n</div>\n\n\n<a id=\"1\"></a>\n<h1 style='background:#00EFFF;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Overview</center></h1>\n\n\n# Overview\n\n**Previously we've implemented our first regression algorithm [Linear Regression](https://www.kaggle.com/code/fareselmenshawii/from-scratch-linear-regression)**\n\n**In this notebook we'll be implementing our first classification algorithm**\n\n**Let's get started**    \n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h1 style='background:#00EFFF;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Imports</center></h1>\n\n# Imports","metadata":{}},{"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport pprint\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:11.925072Z","iopub.execute_input":"2023-09-29T15:21:11.925545Z","iopub.status.idle":"2023-09-29T15:21:11.932081Z","shell.execute_reply.started":"2023-09-29T15:21:11.925515Z","shell.execute_reply":"2023-09-29T15:21:11.930842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h1 style='background:#00EFFF;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Data Loading and Analysis</center></h1>\n\n# Data Loading and Analysis","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/breast-cancer-dataset/breast-cancer.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:12.152365Z","iopub.execute_input":"2023-09-29T15:21:12.153511Z","iopub.status.idle":"2023-09-29T15:21:12.197222Z","shell.execute_reply.started":"2023-09-29T15:21:12.153449Z","shell.execute_reply":"2023-09-29T15:21:12.194921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:12.19994Z","iopub.execute_input":"2023-09-29T15:21:12.20031Z","iopub.status.idle":"2023-09-29T15:21:12.243608Z","shell.execute_reply.started":"2023-09-29T15:21:12.200281Z","shell.execute_reply":"2023-09-29T15:21:12.241643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data_frame=df, x='diagnosis', color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:12.343363Z","iopub.execute_input":"2023-09-29T15:21:12.343719Z","iopub.status.idle":"2023-09-29T15:21:12.403871Z","shell.execute_reply.started":"2023-09-29T15:21:12.34369Z","shell.execute_reply":"2023-09-29T15:21:12.402443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data_frame=df,x='area_mean',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:12.507388Z","iopub.execute_input":"2023-09-29T15:21:12.507794Z","iopub.status.idle":"2023-09-29T15:21:12.560135Z","shell.execute_reply.started":"2023-09-29T15:21:12.507764Z","shell.execute_reply":"2023-09-29T15:21:12.558176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data_frame=df,x='radius_mean',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:12.990628Z","iopub.execute_input":"2023-09-29T15:21:12.990986Z","iopub.status.idle":"2023-09-29T15:21:13.044045Z","shell.execute_reply.started":"2023-09-29T15:21:12.990956Z","shell.execute_reply":"2023-09-29T15:21:13.042661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data_frame=df,x='perimeter_mean',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:13.046192Z","iopub.execute_input":"2023-09-29T15:21:13.046633Z","iopub.status.idle":"2023-09-29T15:21:13.095886Z","shell.execute_reply.started":"2023-09-29T15:21:13.046597Z","shell.execute_reply":"2023-09-29T15:21:13.094898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data_frame=df,x='smoothness_mean',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:13.097744Z","iopub.execute_input":"2023-09-29T15:21:13.098103Z","iopub.status.idle":"2023-09-29T15:21:13.147874Z","shell.execute_reply.started":"2023-09-29T15:21:13.098069Z","shell.execute_reply":"2023-09-29T15:21:13.146149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data_frame=df,x='texture_mean',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:13.149373Z","iopub.execute_input":"2023-09-29T15:21:13.149781Z","iopub.status.idle":"2023-09-29T15:21:13.205372Z","shell.execute_reply.started":"2023-09-29T15:21:13.149751Z","shell.execute_reply":"2023-09-29T15:21:13.204471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.scatter(data_frame=df,x='symmetry_worst',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:13.207499Z","iopub.execute_input":"2023-09-29T15:21:13.207835Z","iopub.status.idle":"2023-09-29T15:21:13.254823Z","shell.execute_reply.started":"2023-09-29T15:21:13.207809Z","shell.execute_reply":"2023-09-29T15:21:13.253967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.scatter(data_frame=df,x='concavity_worst',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:13.255624Z","iopub.execute_input":"2023-09-29T15:21:13.255893Z","iopub.status.idle":"2023-09-29T15:21:13.30323Z","shell.execute_reply.started":"2023-09-29T15:21:13.25587Z","shell.execute_reply":"2023-09-29T15:21:13.30267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.scatter(data_frame=df,x='fractal_dimension_worst',color='diagnosis',color_discrete_sequence=['#05445E','#75E6DA'])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:13.304138Z","iopub.execute_input":"2023-09-29T15:21:13.304478Z","iopub.status.idle":"2023-09-29T15:21:13.347491Z","shell.execute_reply.started":"2023-09-29T15:21:13.304455Z","shell.execute_reply":"2023-09-29T15:21:13.346655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h1 style='background:#00EFFF;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Data Preprocessing</center></h1>\n\n# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"df.drop('id', axis=1, inplace=True) #drop redundant columns\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:13.350483Z","iopub.execute_input":"2023-09-29T15:21:13.350739Z","iopub.status.idle":"2023-09-29T15:21:13.356669Z","shell.execute_reply.started":"2023-09-29T15:21:13.350716Z","shell.execute_reply":"2023-09-29T15:21:13.355607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['diagnosis'] = (df['diagnosis'] == 'M').astype(int) #encode the label into 1/0","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:13.35798Z","iopub.execute_input":"2023-09-29T15:21:13.358266Z","iopub.status.idle":"2023-09-29T15:21:13.374826Z","shell.execute_reply.started":"2023-09-29T15:21:13.358237Z","shell.execute_reply":"2023-09-29T15:21:13.373461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now let's get highly correlated features with the target**","metadata":{}},{"cell_type":"code","source":"corr = df.corr()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:13.376476Z","iopub.execute_input":"2023-09-29T15:21:13.377815Z","iopub.status.idle":"2023-09-29T15:21:13.392106Z","shell.execute_reply.started":"2023-09-29T15:21:13.377766Z","shell.execute_reply":"2023-09-29T15:21:13.391052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(corr, cmap='mako_r',annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:13.394329Z","iopub.execute_input":"2023-09-29T15:21:13.394834Z","iopub.status.idle":"2023-09-29T15:21:15.821Z","shell.execute_reply.started":"2023-09-29T15:21:13.394805Z","shell.execute_reply":"2023-09-29T15:21:15.819915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the absolute value of the correlation\ncor_target = abs(corr[\"diagnosis\"])\n\n# Select highly correlated features (thresold = 0.2)\nrelevant_features = cor_target[cor_target>0.2]\n\n# Collect the names of the features\nnames = [index for index, value in relevant_features.iteritems()]\n\n# Drop the target variable from the results\nnames.remove('diagnosis')\n\n# Display the results\npprint.pprint(names)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:15.822249Z","iopub.execute_input":"2023-09-29T15:21:15.822729Z","iopub.status.idle":"2023-09-29T15:21:15.83167Z","shell.execute_reply.started":"2023-09-29T15:21:15.822689Z","shell.execute_reply":"2023-09-29T15:21:15.830286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df[names].values\ny = df['diagnosis'].values","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:15.833627Z","iopub.execute_input":"2023-09-29T15:21:15.833917Z","iopub.status.idle":"2023-09-29T15:21:15.848244Z","shell.execute_reply.started":"2023-09-29T15:21:15.833893Z","shell.execute_reply":"2023-09-29T15:21:15.847156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_split(X, y, random_state=42, test_size=0.2):\n    \"\"\"\n    Splits the data into training and testing sets.\n\n    Parameters:\n        X (numpy.ndarray): Features array of shape (n_samples, n_features).\n        y (numpy.ndarray): Target array of shape (n_samples,).\n        random_state (int): Seed for the random number generator. Default is 42.\n        test_size (float): Proportion of samples to include in the test set. Default is 0.2.\n\n    Returns:\n        Tuple[numpy.ndarray]: A tuple containing X_train, X_test, y_train, y_test.\n    \"\"\"\n    # Get number of samples\n    n_samples = X.shape[0]\n\n    # Set the seed for the random number generator\n    np.random.seed(random_state)\n\n    # Shuffle the indices\n    shuffled_indices = np.random.permutation(np.arange(n_samples))\n\n    # Determine the size of the test set\n    test_size = int(n_samples * test_size)\n\n    # Split the indices into test and train\n    test_indices = shuffled_indices[:test_size]\n    train_indices = shuffled_indices[test_size:]\n\n    # Split the features and target arrays into test and train\n    X_train, X_test = X[train_indices], X[test_indices]\n    y_train, y_test = y[train_indices], y[test_indices]\n\n    return X_train, X_test, y_train, y_test\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:15.849381Z","iopub.execute_input":"2023-09-29T15:21:15.850342Z","iopub.status.idle":"2023-09-29T15:21:15.86295Z","shell.execute_reply.started":"2023-09-29T15:21:15.850309Z","shell.execute_reply":"2023-09-29T15:21:15.862281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:15.863961Z","iopub.execute_input":"2023-09-29T15:21:15.864656Z","iopub.status.idle":"2023-09-29T15:21:15.882144Z","shell.execute_reply.started":"2023-09-29T15:21:15.864628Z","shell.execute_reply":"2023-09-29T15:21:15.881454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Standardize the data\n\nStandardization is a preprocessing technique used in machine learning to rescale and transform the features (variables) of a dataset to have a mean of 0 and a standard deviation of 1. It is also known as \"z-score normalization\" or \"z-score scaling.\" Standardization is an essential step in the data preprocessing pipeline for various reasons:\n\n### Why Use Standardization in Machine Learning?\n\n1. **Mean Centering**: Standardization centers the data by subtracting the mean from each feature. This ensures that the transformed data has a mean of 0. Mean centering is crucial because it helps in capturing the relative variations in the data.\n\n2. **Scale Invariance**: Standardization scales the data by dividing each feature by its standard deviation. This makes the data scale-invariant, meaning that the scale of the features no longer affects the performance of many machine learning algorithms. Without standardization, features with larger scales may dominate the learning process.\n\n3. **Improved Convergence**: Many machine learning algorithms, such as gradient-based optimization algorithms (e.g., gradient descent), converge faster when the features are standardized. It reduces the potential for numerical instability and overflow/underflow issues during training.\n\n4. **Comparability**: Standardizing the features makes it easier to compare and interpret the importance of each feature. This is especially important in models like linear regression, where the coefficients represent the feature's impact on the target variable.\n\n5. **Regularization**: In regularization techniques like Ridge and Lasso regression, the regularization strength is applied uniformly to all features. Standardization ensures that the regularization term applies fairly to all features.\n\n### How to Standardize Data\n\nThe standardization process involves the following steps:\n\n1. Calculate the mean ($\\mu$) and standard deviation ($\\sigma$) for each feature in the dataset.\n2. For each data point (sample), subtract the mean ($\\mu$) of the feature and then divide by the standard deviation ($\\sigma$) of the feature.\n\nMathematically, the standardized value for a feature `x` in a dataset is calculated as:\n\n$$\n\\text{Standardized value} = \\frac{x - \\mu}{\\sigma}\n$$\n\nHere, `x` is the original value of the feature, $\\mu$ is the mean of the feature, and $\\sigma$ is the standard deviation of the feature.\n","metadata":{}},{"cell_type":"code","source":"def standardize_data(X_train, X_test):\n    \"\"\"\n    Standardizes the input data using mean and standard deviation.\n\n    Parameters:\n        X_train (numpy.ndarray): Training data.\n        X_test (numpy.ndarray): Testing data.\n\n    Returns:\n        Tuple of standardized training and testing data.\n    \"\"\"\n    # Calculate the mean and standard deviation using the training data\n    mean = np.mean(X_train, axis=0)\n    std = np.std(X_train, axis=0)\n    \n    # Standardize the data\n    X_train = (X_train - mean) / std\n    X_test = (X_test - mean) / std\n    \n    return X_train, X_test\n\nX_train, X_test = standardize_data(X_train, X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:21:15.883026Z","iopub.execute_input":"2023-09-29T15:21:15.883801Z","iopub.status.idle":"2023-09-29T15:21:15.898562Z","shell.execute_reply.started":"2023-09-29T15:21:15.883775Z","shell.execute_reply":"2023-09-29T15:21:15.897022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<h1 style='background:#00EFFF;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Model Implementation</center></h1>\n\n# Model Implementation","metadata":{}},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression Model\n\nLogistic regression is a widely used model in machine learning for binary classification tasks. It models the probability that a given input belongs to a particular class. The logistic regression model function is represented as:\n\n$$ f_{\\mathbf{w},b}(\\mathbf{x}) = g(\\mathbf{w} \\cdot \\mathbf{x} + b) $$\n\nIn this equation, $f_{\\mathbf{w},b}(\\mathbf{x})$ represents the predicted probability, $\\mathbf{w}$ is the weight vector, $\\mathbf{b}$ is the bias term, $\\mathbf{x}$ is the input feature vector, and $g(z)$ is the sigmoid function:\n\n$$ g(z) = \\frac{1}{1 + e^{-z}} $$\n\n## Model Training\n\nTo train a logistic regression model, we aim to find the best values for the parameters $(\\mathbf{w}, b)$ that best fit our dataset and provide accurate class probabilities.\n\n### Forward Pass\n\nThe forward pass computes the linear combination of input features $\\mathbf{x}$ with the weight vector $\\mathbf{w}$ and the bias term $b$ and then applies the sigmoid function to the result:\n\n$$ Z = \\mathbf{x} \\cdot \\mathbf{w} + b $$\n$$ A = \\sigma(Z) $$\n\n### Cost Function\n\nThe cost function measures the error between the predicted probabilities and the true labels. In logistic regression, we use the binary cross-entropy loss function:\n\n$$ J(\\mathbf{w},b) = -\\frac{1}{m} \\sum_{i=0}^{m-1} \\left[y_i \\log\\left(f_{\\mathbf{w},b}(\\mathbf{x}_i)\\right) + (1 - y_i) \\log\\left(1 - f_{\\mathbf{w},b}(\\mathbf{x}_i)\\right)\\right] $$\n\nHere, $m$ is the number of samples, $y_i$ is the true label of sample $i$, and $f_{\\mathbf{w},b}(\\mathbf{x}_i)$ is the predicted probability of sample $i$ belonging to the positive class.\n\n### Backward Pass (Gradient Computation)\n\nThe backward pass calculates the gradients of the cost function with respect to the parameters $(\\mathbf{w}, b)$. These gradients are essential for updating the model parameters during training:\n\n$$ \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left(f_{\\mathbf{w},b}(\\mathbf{x}_i) - y_i\\right) $$\n\n$$ \\frac{\\partial J(\\mathbf{w},b)}{\\partial \\mathbf{w}} = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left(f_{\\mathbf{w},b}(\\mathbf{x}_i) - y_i\\right)\\mathbf{x}_i $$\n\n## Training Process\n\nThe training process involves iteratively updating the weight vector $\\mathbf{w}$ and bias term $b$ to minimize the cost function. This is typically done through an optimization algorithm like gradient descent. The update equations for parameters are:\n\n$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{w}} $$\n\n$$ b \\leftarrow b - \\alpha \\frac{\\partial J}{\\partial b} $$\n\nHere, $\\alpha$ represents the learning rate, which controls the step size during parameter updates.\n\nBy iteratively performing the forward pass, computing the cost, performing the backward pass, and updating the parameters, the logistic regression model learns to make better predictions and fit the data.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def sigmoid(z):\n    \"\"\"\n    Compute the sigmoid function for a given input.\n\n    The sigmoid function is a mathematical function used in logistic regression and neural networks\n    to map any real-valued number to a value between 0 and 1.\n\n    Parameters:\n        z (float or numpy.ndarray): The input value(s) for which to compute the sigmoid.\n\n    Returns:\n        float or numpy.ndarray: The sigmoid of the input value(s).\n\n    Example:\n        >>> sigmoid(0)\n        0.5\n    \"\"\"\n    # Compute the sigmoid function using the formula: 1 / (1 + e^(-z)).\n    sigmoid_result = 1 / (1 + np.exp(-z))\n    \n    # Return the computed sigmoid value.\n    return sigmoid_result","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:26:46.456656Z","iopub.execute_input":"2023-09-29T15:26:46.456988Z","iopub.status.idle":"2023-09-29T15:26:46.463106Z","shell.execute_reply.started":"2023-09-29T15:26:46.456963Z","shell.execute_reply":"2023-09-29T15:26:46.461777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualize the shape of the  Logistic function**","metadata":{}},{"cell_type":"code","source":"z = np.linspace(-12, 12, 200)\n\nfig = px.line(x=z, y=sigmoid(z),title='Logistic Function',template=\"plotly_dark\")\nfig.update_layout(\n    title_font_color=\"#41BEE9\", \n    xaxis=dict(color=\"#41BEE9\"), \n    yaxis=dict(color=\"#41BEE9\") \n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:26:47.643459Z","iopub.execute_input":"2023-09-29T15:26:47.643868Z","iopub.status.idle":"2023-09-29T15:26:47.689718Z","shell.execute_reply.started":"2023-09-29T15:26:47.643836Z","shell.execute_reply":"2023-09-29T15:26:47.688513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" class LogisticRegression:\n    \"\"\"\n    Logistic Regression model.\n\n    Parameters:\n        learning_rate (float): Learning rate for the model.\n\n    Methods:\n        initialize_parameter(): Initializes the parameters of the model.\n        sigmoid(z): Computes the sigmoid activation function for given input z.\n        forward(X): Computes forward propagation for given input X.\n        compute_cost(predictions): Computes the cost function for given predictions.\n        compute_gradient(predictions): Computes the gradients for the model using given predictions.\n        fit(X, y, iterations, plot_cost): Trains the model on given input X and labels y for specified iterations.\n        predict(X): Predicts the labels for given input X.\n    \"\"\"\n\n    def __init__(self, learning_rate=0.0001):\n        np.random.seed(1)\n        self.learning_rate = learning_rate\n\n    def initialize_parameter(self):\n        \"\"\"\n        Initializes the parameters of the model.\n        \"\"\"\n        self.W = np.zeros(self.X.shape[1])\n        self.b = 0.0\n\n\n    def forward(self, X):\n        \"\"\"\n        Computes forward propagation for given input X.\n\n        Parameters:\n            X (numpy.ndarray): Input array.\n\n        Returns:\n            numpy.ndarray: Output array.\n        \"\"\"\n#         print(X.shape, self.W.shape)\n        Z = np.matmul(X, self.W) + self.b\n        A = sigmoid(Z)\n        return A\n\n    def compute_cost(self, predictions):\n        \"\"\"\n        Computes the cost function for given predictions.\n\n        Parameters:\n            predictions (numpy.ndarray): Predictions of the model.\n\n        Returns:\n            float: Cost of the model.\n        \"\"\"\n        m = self.X.shape[0]  # number of training examples\n        # compute the cost\n        cost = np.sum((-np.log(predictions + 1e-8) * self.y) + (-np.log(1 - predictions + 1e-8)) * (\n                1 - self.y))  # we are adding small value epsilon to avoid log of 0\n        cost = cost / m\n        return cost\n\n    def compute_gradient(self, predictions):\n        \"\"\"\n        Computes the gradients for the model using given predictions.\n\n        Parameters:\n            predictions (numpy.ndarray): Predictions of the model.\n        \"\"\"\n        # get training shape\n        m = self.X.shape[0]\n\n        # compute gradients\n        self.dW = np.matmul(self.X.T, (predictions - self.y))\n        self.dW = np.array([np.mean(grad) for grad in self.dW])\n\n        self.db = np.sum(np.subtract(predictions, self.y))\n\n        # scale gradients\n        self.dW = self.dW * 1 / m\n        self.db = self.db * 1 / m\n\n\n    def fit(self, X, y, iterations, plot_cost=True):\n        \"\"\"\n        Trains the model on given input X and labels y for specified iterations.\n\n        Parameters:\n            X (numpy.ndarray): Input features array of shape (n_samples, n )\n            y (numpy.ndarray): Labels array of shape (n_samples, 1)\n            iterations (int): Number of iterations for training.\n            plot_cost (bool): Whether to plot cost over iterations or not.\n\n        Returns:\n            None.\n        \"\"\"\n        self.X = X\n        self.y = y\n\n        self.initialize_parameter()\n\n        costs = []\n        for i in range(iterations):\n            # forward propagation\n            predictions = self.forward(self.X)\n\n            # compute cost\n            cost = self.compute_cost(predictions)\n            costs.append(cost)\n\n            # compute gradients\n            self.compute_gradient(predictions)\n\n            # update parameters\n            self.W = self.W - self.learning_rate * self.dW\n            self.b = self.b - self.learning_rate * self.db\n\n            # print cost every 100 iterations\n            if i % 10000 == 0:\n                print(\"Cost after iteration {}: {}\".format(i, cost))\n\n        if plot_cost:\n            fig = px.line(y=costs,title=\"Cost vs Iteration\",template=\"plotly_dark\")\n            fig.update_layout(\n                title_font_color=\"#41BEE9\", \n                xaxis=dict(color=\"#41BEE9\",title=\"Iterations\"), \n                yaxis=dict(color=\"#41BEE9\",title=\"cost\")\n            )\n            fig.show()\n\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the labels for given input X.\n\n        Parameters:\n            X (numpy.ndarray): Input features array.\n\n        Returns:\n            numpy.ndarray: Predicted labels.\n        \"\"\"\n        predictions = self.forward(X)\n        return np.round(predictions)\n    \n    \n    def save_model(self, filename=None):\n        \"\"\"\n        Save the trained model to a file using pickle.\n\n        Parameters:\n            filename (str): The name of the file to save the model to.\n        \"\"\"\n        model_data = {\n            'learning_rate': self.learning_rate,\n            'W': self.W,\n            'b': self.b\n        }\n\n        with open(filename, 'wb') as file:\n            pickle.dump(model_data, file)\n\n    @classmethod\n    def load_model(cls, filename):\n        \"\"\"\n        Load a trained model from a file using pickle.\n\n        Parameters:\n            filename (str): The name of the file to load the model from.\n\n        Returns:\n            LogisticRegression: An instance of the LogisticRegression class with loaded parameters.\n        \"\"\"\n        with open(filename, 'rb') as file:\n            model_data = pickle.load(file)\n\n        # Create a new instance of the class and initialize it with the loaded parameters\n        loaded_model = cls(model_data['learning_rate'])\n        loaded_model.W = model_data['W']\n        loaded_model.b = model_data['b']\n\n        return loaded_model","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:36:27.008042Z","iopub.execute_input":"2023-09-29T15:36:27.008365Z","iopub.status.idle":"2023-09-29T15:36:27.028795Z","shell.execute_reply.started":"2023-09-29T15:36:27.008342Z","shell.execute_reply":"2023-09-29T15:36:27.027372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg = LogisticRegression()\nlg.fit(X_train, y_train,100000)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:36:27.20724Z","iopub.execute_input":"2023-09-29T15:36:27.207608Z","iopub.status.idle":"2023-09-29T15:37:01.597214Z","shell.execute_reply.started":"2023-09-29T15:36:27.207569Z","shell.execute_reply":"2023-09-29T15:37:01.596281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg.save_model(\"model.pkl\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:37:01.598527Z","iopub.execute_input":"2023-09-29T15:37:01.599468Z","iopub.status.idle":"2023-09-29T15:37:01.603029Z","shell.execute_reply.started":"2023-09-29T15:37:01.599438Z","shell.execute_reply":"2023-09-29T15:37:01.60236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n<h1 style='background:#00EFFF;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Evaluation</center></h1>\n    \n    \n    \n# Evaluation\n","metadata":{}},{"cell_type":"markdown","source":"\n**In classification tasks, it's crucial to evaluate the performance of your model. There are several metrics that can help you understand how well your model is performing. Here are four commonly used classification metrics:**\n## 1. Accuracy\n\n**Formula:**\n$$\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n$$\n\n**Description:**\n- **Accuracy** measures the proportion of correctly predicted instances out of all instances in a classification model.\n- It is a widely used metric for evaluating classification performance.\n\n**Interpretation:**\n- A higher accuracy value indicates a better classification model.\n- However, accuracy alone may not provide a complete picture, especially in imbalanced datasets.\n\n## 2. Precision\n\n**Formula:**\n$$\n\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\n$$\n\n**Description:**\n- **Precision** measures the proportion of true positive predictions out of all positive predictions made by the model.\n- It is a useful metric when the cost of false positives is high.\n\n**Interpretation:**\n- Higher precision means the model makes fewer false positive predictions.\n\n## 3. Recall (Sensitivity)\n\n**Formula:**\n$$\n\\text{Recall (Sensitivity)} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\n$$\n\n**Description:**\n- **Recall**, also known as sensitivity or true positive rate, measures the proportion of true positive predictions out of all actual positive instances in the dataset.\n- It is a valuable metric when it's essential to capture all positive instances.\n\n**Interpretation:**\n- Higher recall means the model captures more of the actual positive instances.\n\n## 4. F1-Score\n\n**Formula:**\n$$\n\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\n$$\n\n**Description:**\n- The **F1-Score** is the harmonic mean of precision and recall.\n- It provides a balance between precision and recall, making it a suitable metric when there is a trade-off between false positives and false negatives.\n\n**Interpretation:**\n- A higher F1-Score indicates a model that achieves a balance between precision and recall.\n\n","metadata":{}},{"cell_type":"code","source":"class ClassificationMetrics:\n    @staticmethod\n    def accuracy(y_true, y_pred):\n        \"\"\"\n        Computes the accuracy of a classification model.\n\n        Parameters:\n        y_true (numpy array): A numpy array of true labels for each data point.\n        y_pred (numpy array): A numpy array of predicted labels for each data point.\n\n        Returns:\n        float: The accuracy of the model, expressed as a percentage.\n        \"\"\"\n        y_true = y_true.flatten()\n        total_samples = len(y_true)\n        correct_predictions = np.sum(y_true == y_pred)\n        return (correct_predictions / total_samples)\n\n    @staticmethod\n    def precision(y_true, y_pred):\n        \"\"\"\n        Computes the precision of a classification model.\n\n        Parameters:\n        y_true (numpy array): A numpy array of true labels for each data point.\n        y_pred (numpy array): A numpy array of predicted labels for each data point.\n\n        Returns:\n        float: The precision of the model, which measures the proportion of true positive predictions\n        out of all positive predictions made by the model.\n        \"\"\"\n        true_positives = np.sum((y_true == 1) & (y_pred == 1))\n        false_positives = np.sum((y_true == 0) & (y_pred == 1))\n        return true_positives / (true_positives + false_positives)\n\n    @staticmethod\n    def recall(y_true, y_pred):\n        \"\"\"\n        Computes the recall (sensitivity) of a classification model.\n\n        Parameters:\n        y_true (numpy array): A numpy array of true labels for each data point.\n        y_pred (numpy array): A numpy array of predicted labels for each data point.\n\n        Returns:\n        float: The recall of the model, which measures the proportion of true positive predictions\n        out of all actual positive instances in the dataset.\n        \"\"\"\n        true_positives = np.sum((y_true == 1) & (y_pred == 1))\n        false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n        return true_positives / (true_positives + false_negatives)\n\n    @staticmethod\n    def f1_score(y_true, y_pred):\n        \"\"\"\n        Computes the F1-score of a classification model.\n\n        Parameters:\n        y_true (numpy array): A numpy array of true labels for each data point.\n        y_pred (numpy array): A numpy array of predicted labels for each data point.\n\n        Returns:\n        float: The F1-score of the model, which is the harmonic mean of precision and recall.\n        \"\"\"\n        precision_value = ClassificationMetrics.precision(y_true, y_pred)\n        recall_value = ClassificationMetrics.recall(y_true, y_pred)\n        return 2 * (precision_value * recall_value) / (precision_value + recall_value)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:37:01.60391Z","iopub.execute_input":"2023-09-29T15:37:01.604587Z","iopub.status.idle":"2023-09-29T15:37:01.621911Z","shell.execute_reply.started":"2023-09-29T15:37:01.60456Z","shell.execute_reply":"2023-09-29T15:37:01.62043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LogisticRegression.load_model(\"model.pkl\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:37:01.624504Z","iopub.execute_input":"2023-09-29T15:37:01.624946Z","iopub.status.idle":"2023-09-29T15:37:01.641932Z","shell.execute_reply.started":"2023-09-29T15:37:01.624909Z","shell.execute_reply":"2023-09-29T15:37:01.640247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\naccuracy = ClassificationMetrics.accuracy(y_test, y_pred)\nprecision = ClassificationMetrics.precision(y_test, y_pred)\nrecall = ClassificationMetrics.recall(y_test, y_pred)\nf1_score = ClassificationMetrics.f1_score(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.2%}\")\nprint(f\"Precision: {precision:.2%}\")\nprint(f\"Recall: {recall:.2%}\")\nprint(f\"F1-Score: {f1_score:.2%}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:37:01.64352Z","iopub.execute_input":"2023-09-29T15:37:01.643962Z","iopub.status.idle":"2023-09-29T15:37:01.659602Z","shell.execute_reply.started":"2023-09-29T15:37:01.643899Z","shell.execute_reply":"2023-09-29T15:37:01.658129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n<h1 style='background:#00EFFF;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Thank You</center></h1>\n    \n    \n    \n\n# Thank You\n\n**Thank you for going through the notebook if you haev any feedback please let me know**\n\n\n**We were able to implement Logistic Regression from scratch**\n\n\n**Thank you for going through this notebook please leave your feedback**\n\n**For Logistic Regression sklearn implementation please refer to this [notebook](https://www.kaggle.com/code/fareselmenshawii/logistic-regression)**\n\n**Now that we know how to implement Linear Regression from scratch let's check [sklearn implementation](https://www.kaggle.com/code/fareselmenshawii/linear-regression)**\n\n","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"<div style=\"padding:10px; \n            color:#333333;\n            margin:10px;\n            font-size:150%;\n            display:fill;\n            border-radius:1px;\n            border-style:solid;\n            border-color:#666666;\n            background-color:#F9F9F9;\n            overflow:hidden;\">\n    <center>\n        <a id='top'></a>\n        <b>Machine Learning From Scratch Series</b>\n    </center>\n    <br>\n    <ul>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/linear-regression-from-scratch\" style=\"color:#0072B2\">1 - Linear Regression</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/logistic-regression-from-scratch\" style=\"color:#0072B2\">2 -  Logistic Regression</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/kmeans-from-scratch\" style=\"color:#0072B2\">3 - KMeans</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/decision-tree-classifier-from-scratch\" style=\"color:#0072B2\">4 - Decision Trees</a>\n        </li> \n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/random-forest-classifier-from-scratch\" style=\"color:#0072B2\">5 -  Random Forest</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/knn-from-scratch\" style=\"color:#0072B2\">6 - KNearestNeighbor</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/pca-from-scratch?scriptVersionId=121402593\" style=\"color:#0072B2\">7 - PCA</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/svm-from-scratch\" style=\"color:#0072B2\">8 - SVM</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/naive-bayes-from-scratch\" style=\"color:#0072B2\">9 - Naive Baye</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/optimized-neural-network-from-scratch\" style=\"color:#0072B2\">10 - Optimized Neural Network</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/neural-network-from-scratch\" style=\"color:#0072B2\">11 - Neural Network</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/cnn-from-scratch\" style=\"color:#0072B2\">12 - CNN</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/rnn-from-scratch\" style=\"color:#0072B2\">13 - RNN</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/lstm-from-scratch\" style=\"color:#0072B2\">14 - LSTM</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/gru-from-scratch\" style=\"color:#0072B2\">15 - GRU</a>\n        </li>\n    </ul>\n</div>","metadata":{}}]}