{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fareselmenshawii/kmeans-from-scratch?scriptVersionId=144704270\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"padding:10px; \n            color:black;\n            margin:10px;\n            font-size:150%;\n            display:block;\n            border-radius:1px;\n            border-style: solid;\n            border-color:AF4BCE;\n            background-color:#AF4BCE;\n            overflow:hidden;\">\n    <center>\n        <a id='top'></a>\n        <b style=\"color:black\">Table of Contents</b>\n    </center>\n    <ul>\n        <li>\n            <a href=\"#1\" style=\"color:black\">1 -  Overview</a>\n        </li>\n        <li>\n            <a href=\"#2\" style=\"color:black\">2 -  Imports</a>\n        </li>\n        <li>\n            <a href=\"#3\" style=\"color:black\">3 - Data Loading and Analysis</a>\n        </li>\n        <li>\n            <a href=\"#4\" style=\"color:black\">4 - Model Implementation </a>\n        </li>\n        <li>\n            <a href=\"#5\" style=\"color:black\">5 - Evaluation</a>\n        </li>\n        <li>\n            <a href=\"#6\" style=\"color:black\">6 - Thank you</a>\n        </li>\n    </ul>\n</div>\n\n\n<a id=\"1\"></a>\n<h1 style='background:#AF4BCE;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Overview</center></h1>\n\n\n# Overview\n\n  \n**Previously we've implemented our first classification algorithm [logistic regression](https://www.kaggle.com/code/fareselmenshawii/logistic-regression-from-scratch)**\n    \n**Now we'll implement our first Clustering Algorithm: KMeans**\n    \n**The K-means algorithm is a method to automatically cluster similar data points together more on that on the model part**\n     \n    \n **We'll be using KMeans to cluster the famous Iris Dataset**\n    \n**Let's get started !**\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h1 style='background:#AF4BCE;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center>Imports</center></h1>\n\n# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nimport plotly.graph_objects as go","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:15.627981Z","iopub.execute_input":"2023-09-29T19:08:15.628436Z","iopub.status.idle":"2023-09-29T19:08:17.479181Z","shell.execute_reply.started":"2023-09-29T19:08:15.628388Z","shell.execute_reply":"2023-09-29T19:08:17.477734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h1 style='background:#AF4BCE;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center>Data Loading and Analysis</center></h1>\n\n# Data Loading and Analysis","metadata":{}},{"cell_type":"code","source":"iris = pd.read_csv(\"../input/iris/Iris.csv\") #Load Data\niris.drop('Id',inplace=True,axis=1) #Drop Id column","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:18.503468Z","iopub.execute_input":"2023-09-29T19:08:18.503893Z","iopub.status.idle":"2023-09-29T19:08:18.542054Z","shell.execute_reply.started":"2023-09-29T19:08:18.503858Z","shell.execute_reply":"2023-09-29T19:08:18.541206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = iris.iloc[:,:-1] #Set our training data\n\ny = iris.iloc[:,-1] #We'll use this just for visualization as clustering doesn't require labels","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:18.54689Z","iopub.execute_input":"2023-09-29T19:08:18.547254Z","iopub.status.idle":"2023-09-29T19:08:18.552883Z","shell.execute_reply.started":"2023-09-29T19:08:18.547222Z","shell.execute_reply":"2023-09-29T19:08:18.551758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iris.head().style.background_gradient(cmap =sns.cubehelix_palette(as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:18.57906Z","iopub.execute_input":"2023-09-29T19:08:18.579499Z","iopub.status.idle":"2023-09-29T19:08:18.659049Z","shell.execute_reply.started":"2023-09-29T19:08:18.579463Z","shell.execute_reply":"2023-09-29T19:08:18.657829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Distribution","metadata":{}},{"cell_type":"code","source":"fig = px.pie(iris, 'Species',color_discrete_sequence=['#491D8B','#7D3AC1','#EB548C'],title='Data Distribution',template='plotly')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:18.660804Z","iopub.execute_input":"2023-09-29T19:08:18.661137Z","iopub.status.idle":"2023-09-29T19:08:19.840913Z","shell.execute_reply.started":"2023-09-29T19:08:18.661108Z","shell.execute_reply":"2023-09-29T19:08:19.839693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## From this plot we conclude that:\n\n**The Data is perfectly balanced**","metadata":{}},{"cell_type":"markdown","source":"****\n","metadata":{}},{"cell_type":"markdown","source":"## Sepal-Length","metadata":{}},{"cell_type":"code","source":"fig = px.box(data_frame=iris, x='Species',y='SepalLengthCm',color='Species',color_discrete_sequence=['#29066B','#7D3AC1','#EB548C'],orientation='v')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:19.842949Z","iopub.execute_input":"2023-09-29T19:08:19.843289Z","iopub.status.idle":"2023-09-29T19:08:20.092575Z","shell.execute_reply.started":"2023-09-29T19:08:19.843258Z","shell.execute_reply":"2023-09-29T19:08:20.091506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(data_frame=iris, x='SepalLengthCm',color='Species',color_discrete_sequence=['#491D8B','#7D3AC1','#EB548C'],nbins=50)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:20.095252Z","iopub.execute_input":"2023-09-29T19:08:20.09581Z","iopub.status.idle":"2023-09-29T19:08:20.181872Z","shell.execute_reply.started":"2023-09-29T19:08:20.095759Z","shell.execute_reply":"2023-09-29T19:08:20.181138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From these plots we conclude that: \n\n* **Setosa has much smaller SepalLength than the other 2 classes**\n\n* **Virginca has the highest SepalLength, however It seems hard to distingush between Virginca and Versicolor using SepalLength as the difference is less clear**\n\n* **We can see that Virginica contains an outlier**","metadata":{}},{"cell_type":"markdown","source":"****","metadata":{}},{"cell_type":"markdown","source":"## SepalWidth","metadata":{}},{"cell_type":"code","source":"fig = px.box(data_frame=iris, x='Species',y='SepalWidthCm',color='Species',color_discrete_sequence=['#29066B','#7D3AC1','#EB548C'],orientation='v')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:20.183595Z","iopub.execute_input":"2023-09-29T19:08:20.184062Z","iopub.status.idle":"2023-09-29T19:08:20.239717Z","shell.execute_reply.started":"2023-09-29T19:08:20.184032Z","shell.execute_reply":"2023-09-29T19:08:20.238725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(data_frame=iris, x='SepalWidthCm',color='Species',color_discrete_sequence=['#491D8B','#7D3AC1','#EB548C'],nbins=30)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:20.241063Z","iopub.execute_input":"2023-09-29T19:08:20.241432Z","iopub.status.idle":"2023-09-29T19:08:20.301154Z","shell.execute_reply.started":"2023-09-29T19:08:20.241388Z","shell.execute_reply":"2023-09-29T19:08:20.300146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From these plots we conclude that: \n\n* **Setosa has  larger SepalWidth than the other 2 classes**\n\n* **Versicolo has smaller SepalWidth than the other 2 classes**\n\n* **Overall all classes seem to have relatively close value of sepalwidth which indicate that is might not be a very useful feature**","metadata":{}},{"cell_type":"markdown","source":"****","metadata":{}},{"cell_type":"markdown","source":"## Petal-Length","metadata":{}},{"cell_type":"code","source":"fig = px.box(data_frame=iris, x='Species',y='PetalLengthCm',color='Species',color_discrete_sequence=['#29066B','#7D3AC1','#EB548C'],orientation='v')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:20.302754Z","iopub.execute_input":"2023-09-29T19:08:20.303584Z","iopub.status.idle":"2023-09-29T19:08:20.360481Z","shell.execute_reply.started":"2023-09-29T19:08:20.303552Z","shell.execute_reply":"2023-09-29T19:08:20.359195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(data_frame=iris, x='PetalLengthCm',color='Species',color_discrete_sequence=['#491D8B','#7D3AC1','#EB548C'],nbins=30)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:20.362273Z","iopub.execute_input":"2023-09-29T19:08:20.36286Z","iopub.status.idle":"2023-09-29T19:08:20.423283Z","shell.execute_reply.started":"2023-09-29T19:08:20.362827Z","shell.execute_reply":"2023-09-29T19:08:20.422458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From these plots we conclude that: \n\n* **Setosa has much smaller PetaLength than the other 2 classes**\n\n* **This difference is less clear between Virginica and Versicolor**\n\n* **Overall this seems like an  PetaLength interesting feature**","metadata":{}},{"cell_type":"markdown","source":"****","metadata":{}},{"cell_type":"markdown","source":"## Petal-Width","metadata":{}},{"cell_type":"code","source":"fig = px.box(data_frame=iris, x='Species',y='PetalWidthCm',color='Species',color_discrete_sequence=['#29066B','#7D3AC1','#EB548C'],orientation='v')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:21.643095Z","iopub.execute_input":"2023-09-29T19:08:21.644091Z","iopub.status.idle":"2023-09-29T19:08:21.706698Z","shell.execute_reply.started":"2023-09-29T19:08:21.644052Z","shell.execute_reply":"2023-09-29T19:08:21.705431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(data_frame=iris, x='PetalWidthCm',color='Species',color_discrete_sequence=['#491D8B','#7D3AC1','#EB548C'],nbins=30)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:23.75474Z","iopub.execute_input":"2023-09-29T19:08:23.755124Z","iopub.status.idle":"2023-09-29T19:08:23.817741Z","shell.execute_reply.started":"2023-09-29T19:08:23.755095Z","shell.execute_reply":"2023-09-29T19:08:23.816632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From these plots we conclude that: \n\n* **Setosa has much smaller PetalWidth than the other 2 classes**\n\n* **This difference is less clear between Virginica and Versicolor**\n\n* **Overall this seems like an  PetalWidth interesting feature**","metadata":{}},{"cell_type":"markdown","source":"****","metadata":{}},{"cell_type":"code","source":"fig = px.scatter(data_frame=iris, x='SepalLengthCm',y='SepalWidthCm'\n           ,color='Species',size='PetalLengthCm',template='seaborn',color_discrete_sequence=['#491D8B','#7D3AC1','#EB548C'],)\n\nfig.update_layout(width=800, height=600,\n                  xaxis=dict(color=\"#BF40BF\"),\n                 yaxis=dict(color=\"#BF40BF\"))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:25.097232Z","iopub.execute_input":"2023-09-29T19:08:25.097638Z","iopub.status.idle":"2023-09-29T19:08:25.281616Z","shell.execute_reply.started":"2023-09-29T19:08:25.097607Z","shell.execute_reply":"2023-09-29T19:08:25.280347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter(data_frame=iris, x='PetalLengthCm',y='PetalWidthCm'\n           ,color='Species',size='SepalLengthCm',template='seaborn',color_discrete_sequence=['#491D8B','#7D3AC1','#EB548C'],)\n\nfig.update_layout(width=800, height=600,\n                  xaxis=dict(color=\"#BF40BF\"),\n                 yaxis=dict(color=\"#BF40BF\"))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:08:25.378434Z","iopub.execute_input":"2023-09-29T19:08:25.378894Z","iopub.status.idle":"2023-09-29T19:08:25.451699Z","shell.execute_reply.started":"2023-09-29T19:08:25.378859Z","shell.execute_reply":"2023-09-29T19:08:25.450467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<h1 style='background:#AF4BCE;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center>Model Implementation</center></h1>\n\n# Model Implementation","metadata":{}},{"cell_type":"markdown","source":"# K-means Clustering\n\n**K-means clustering is a method used to group data points into clusters. It operates through an iterative process as follows:**\n\n## Initialization\n- **We begin by randomly selecting initial cluster centroids. These centroids are points in our dataset that will** **act as the centers of our clusters.**\n\n## Assignment of Points to Centroids\n- **For each data point in our training set, we determine which centroid it is closest to. This is done b0y** **measuring the distance between the data point and each centroid and selecting the centroid with the smallest** **distance as the closest one. We use an index notation, denoted as $c^{(i)}$, to represent the index of the** **closest centroid to the data point $x^{(i)}$.**\n\n## Computing Cluster Means\n- **After assigning data points to their closest centroids, we calculate the mean of all data points within each** **cluster. This mean becomes the new centroid for that cluster.**\n\n**The goal of K-means is to minimize a cost function, which is typically the squared Euclidean distance between** **each data point and its assigned centroid:**\n\n$$\n\\sum_{i=1}^{m} ||x^{(i)} - \\mu_j||^2\n$$\n\n**Where:**\n- **$m$ is the number of data points.**\n- **$x^{(i)}$ represents the i-th data point.**\n- **$\\mu_j$ represents the centroid of the j-th cluster.**\n\n**The algorithm repeats the assignment and recomputation of centroids until convergence, where the centroids no** **longer change significantly or a specified number of iterations is reached.**\n\n**To ensure efficient execution and avoid using explicit for-loops, it's recommended to use  [vectorized code](https://www.kaggle.com/code/fareselmenshawii/vectorization) code techniques. This helps improve the performance of the K-means algorithm, especially when working with large datasets.**\n\n","metadata":{}},{"cell_type":"code","source":"class Kmeans:\n    \"\"\"\n    K-Means clustering algorithm implementation.\n\n    Parameters:\n        K (int): Number of clusters\n\n    Attributes:\n        K (int): Number of clusters\n        centroids (numpy.ndarray): Array containing the centroids of each cluster\n\n    Methods:\n        __init__(self, K): Initializes the Kmeans instance with the specified number of clusters.\n        initialize_centroids(self, X): Initializes the centroids for each cluster by selecting K random points from the dataset.\n        assign_points_centroids(self, X): Assigns each point in the dataset to the nearest centroid.\n        compute_mean(self, X, points): Computes the mean of the points assigned to each centroid.\n        fit(self, X, iterations=10): Clusters the dataset using the K-Means algorithm.\n    \"\"\"\n    \n    def __init__(self, K):\n        assert K > 0, \"K should be a positive integer.\"\n        self.K = K\n        \n    def initialize_centroids(self, X):\n        assert X.shape[0] >= self.K, \"Number of data points should be greater than or equal to K.\"\n        \n        randomized_X = np.random.permutation(X.shape[0]) \n        centroid_idx = randomized_X[:self.K] # get the indices for the centroids\n        self.centroids = X[centroid_idx] # assign the centroids to the selected points\n        \n    def assign_points_centroids(self, X):\n        \"\"\"\n        Assign each point in the dataset to the nearest centroid.\n        \n        Parameters:\n        X (numpy.ndarray): dataset to cluster\n        \n        Returns:\n        numpy.ndarray: array containing the index of the centroid for each point\n        \"\"\"\n        X = np.expand_dims(X, axis=1) # expand dimensions to match shape of centroids\n        distance = np.linalg.norm((X - self.centroids), axis=-1) # calculate Euclidean distance between each point and each centroid\n        points = np.argmin(distance, axis=1) # assign each point to the closest centroid\n        assert len(points) == X.shape[0], \"Number of assigned points should equal the number of data points.\"\n        return points\n    \n    def compute_mean(self, X, points):\n        \"\"\"\n        Compute the mean of the points assigned to each centroid.\n        \n        Parameters:\n        X (numpy.ndarray): dataset to cluster\n        points (numpy.ndarray): array containing the index of the centroid for each point\n        \n        Returns:\n        numpy.ndarray: array containing the new centroids for each cluster\n        \"\"\"\n        centroids = np.zeros((self.K, X.shape[1])) # initialize array to store centroids\n        for i in range(self.K):\n            centroid_mean = X[points == i].mean(axis=0) # calculate mean of the points assigned to the current centroid\n            centroids[i] = centroid_mean # assign the new centroid to the mean of its points\n        return centroids\n    \n    def fit(self, X, iterations=10):\n        \"\"\"\n        Cluster the dataset using the K-Means algorithm.\n        \n        Parameters:\n        X (numpy.ndarray): dataset to cluster\n        iterations (int): number of iterations to perform (default=10)\n        \n        Returns:\n        numpy.ndarray: array containing the final centroids for each cluster\n        numpy.ndarray: array containing the index of the centroid for each point\n        \"\"\"\n        self.initialize_centroids(X) # initialize the centroids\n        for i in range(iterations):\n            points = self.assign_points_centroids(X) # assign each point to the nearest centroid\n            self.centroids = self.compute_mean(X, points) # compute the new centroids based on the mean of their points\n            \n            # Assertions for debugging and validation\n            assert len(self.centroids) == self.K, \"Number of centroids should equal K.\"\n            assert X.shape[1] == self.centroids.shape[1], \"Dimensionality of centroids should match input data.\"\n            assert max(points) < self.K, \"Cluster index should be less than K.\"\n            assert min(points) >= 0, \"Cluster index should be non-negative.\"\n            \n        return self.centroids, points\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:11:04.106045Z","iopub.execute_input":"2023-09-29T19:11:04.106486Z","iopub.status.idle":"2023-09-29T19:11:04.119631Z","shell.execute_reply.started":"2023-09-29T19:11:04.106452Z","shell.execute_reply":"2023-09-29T19:11:04.118098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now let's run KMeans for 1000 iterations","metadata":{}},{"cell_type":"code","source":"X = X.values","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:11:06.632186Z","iopub.execute_input":"2023-09-29T19:11:06.633225Z","iopub.status.idle":"2023-09-29T19:11:06.639102Z","shell.execute_reply.started":"2023-09-29T19:11:06.633162Z","shell.execute_reply":"2023-09-29T19:11:06.637832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = Kmeans(3)\n\ncentroids, points = kmeans.fit(X, 1000)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:11:29.815425Z","iopub.execute_input":"2023-09-29T19:11:29.815813Z","iopub.status.idle":"2023-09-29T19:11:29.95058Z","shell.execute_reply.started":"2023-09-29T19:11:29.815785Z","shell.execute_reply":"2023-09-29T19:11:29.94967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n<h1 style='background:#AF4BCE;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center>Evaluation</center></h1>\n\n# Evaluation\n\n**Now let's visualize our results**","metadata":{}},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=X[points == 0, 0], y=X[points == 0, 1],\n    mode='markers',marker_color='#DB4CB2',name='Iris-setosa'\n))\n\nfig.add_trace(go.Scatter(\n    x=X[points == 1, 0], y=X[points == 1, 1],\n    mode='markers',marker_color='#c9e9f6',name='Iris-versicolour'\n))\n\nfig.add_trace(go.Scatter(\n    x=X[points == 2, 0], y=X[points == 2, 1],\n    mode='markers',marker_color='#7D3AC1',name='Iris-virginica'\n))\n\nfig.add_trace(go.Scatter(\n    x=centroids[:, 0], y=centroids[:,1],\n    mode='markers',marker_color='#CAC9CD',marker_symbol=4,marker_size=13,name='Centroids'\n))\nfig.update_layout(template='plotly_dark',width=1000, height=500,)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T19:11:30.372492Z","iopub.execute_input":"2023-09-29T19:11:30.372916Z","iopub.status.idle":"2023-09-29T19:11:30.451492Z","shell.execute_reply.started":"2023-09-29T19:11:30.372882Z","shell.execute_reply":"2023-09-29T19:11:30.450433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n<h1 style='background:#AF4BCE;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center>Thank You</center></h1>\n\n# Thank You\n\n\n**Thank you for going through this notebook**\n\n**If you have any feedback please let me know**\n\n**For KMeans from Sklearn Implementation please refer to this [notebook](https://www.kaggle.com/code/fareselmenshawii/kmeans-iris-clustering)**","metadata":{}},{"cell_type":"markdown","source":"<div style=\"padding:10px; \n            color:#333333;\n            margin:10px;\n            font-size:150%;\n            display:fill;\n            border-radius:1px;\n            border-style:solid;\n            border-color:#666666;\n            background-color:#F9F9F9;\n            overflow:hidden;\">\n    <center>\n        <a id='top'></a>\n        <b>Machine Learning From Scratch Series</b>\n    </center>\n    <br>\n    <ul>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/linear-regression-from-scratch\" style=\"color:#0072B2\">1 - Linear Regression</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/logistic-regression-from-scratch\" style=\"color:#0072B2\">2 -  Logistic Regression</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/kmeans-from-scratch\" style=\"color:#0072B2\">3 - KMeans</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/decision-tree-classifier-from-scratch\" style=\"color:#0072B2\">4 - Decision Trees</a>\n        </li> \n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/random-forest-classifier-from-scratch\" style=\"color:#0072B2\">5 -  Random Forest</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/knn-from-scratch\" style=\"color:#0072B2\">6 - KNearestNeighbor</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/pca-from-scratch?scriptVersionId=121402593\" style=\"color:#0072B2\">7 - PCA</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/svm-from-scratch\" style=\"color:#0072B2\">8 - SVM</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/naive-bayes-from-scratch\" style=\"color:#0072B2\">9 - Naive Baye</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/optimized-neural-network-from-scratch\" style=\"color:#0072B2\">10 - Optimized Neural Network</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/neural-network-from-scratch\" style=\"color:#0072B2\">11 - Neural Network</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/cnn-from-scratch\" style=\"color:#0072B2\">12 - CNN</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/rnn-from-scratch\" style=\"color:#0072B2\">13 - RNN</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/lstm-from-scratch\" style=\"color:#0072B2\">14 - LSTM</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/gru-from-scratch\" style=\"color:#0072B2\">15 - GRU</a>\n        </li>\n    </ul>\n</div>","metadata":{}}]}