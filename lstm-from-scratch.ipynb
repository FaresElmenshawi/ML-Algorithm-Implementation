{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fareselmenshawii/lstm-from-scratch?scriptVersionId=121774265\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","id":"2a9ee720","metadata":{"papermill":{"duration":0.00528,"end_time":"2023-03-11T13:38:58.964669","exception":false,"start_time":"2023-03-11T13:38:58.959389","status":"completed"},"tags":[]},"source":["<div style=\"padding:10px; \n","            color:#FF9F00;\n","            margin:10px;\n","            font-size:150%;\n","            display:fill;\n","            border-radius:1px;\n","            border-style: solid;\n","            border-color:#FF9F00;\n","            background-color:#3E3D53;\n","            overflow:hidden;\">\n","    <center>\n","        <a id='top'></a>\n","        <b>Table of Contents</b>\n","    </center>\n","    <br>\n","    <ul>\n","        <li>\n","            <a href=\"#1\">1 -  Overview and Imports</a>\n","        </li>\n","        <li>\n","            <a href=\"#2\">2 - Data Preparation</a>\n","        </li>\n","        <li>\n","            <a href=\"#3\">3 - LSTM Implementation</a>\n","        <li>\n","            <a href=\"#4\">4 - Thank you</a>\n","        </li> \n","        <li>\n","            <a href=\"#5\">5 - References</a>\n","        </li>\n","    </ul>\n","</div>\n","<a id=\"1\"></a>\n","\n","<h1 style='background:#FF9F00;border:0; color:black;\n","    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n","    transform: rotateX(10deg);\n","    '><center style='color: #3E3D53;'>Overview and Imports</center></h1>\n","    \n","# Overview and Imports\n","\n","**Long Short-Term Memory (LSTM) networks are a type of Recurrent Neural Network (RNN) that are designed to handle sequential data, such as time series or natural language text. They achieve this by using a memory cell that can selectively remember or forget information at each time step of the input sequence, allowing the network to maintain a memory of previous inputs over a longer period of time.**\n","\n","**This notebook contains an implementation of an LSTM that can be used for language modeling. The self takes in a sequence of characters and outputs the probability distribution over the next character in the sequence. The network is trained on a corpus of text and then used to generate new text that has a similar distribution of characters as the training corpus.**"]},{"cell_type":"code","execution_count":1,"id":"730cab59","metadata":{"execution":{"iopub.execute_input":"2023-03-11T13:38:58.974981Z","iopub.status.busy":"2023-03-11T13:38:58.974422Z","iopub.status.idle":"2023-03-11T13:38:59.005383Z","shell.execute_reply":"2023-03-11T13:38:59.004166Z"},"papermill":{"duration":0.03936,"end_time":"2023-03-11T13:38:59.008184","exception":false,"start_time":"2023-03-11T13:38:58.968824","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","import numpy as np\n","import scipy as sp"]},{"cell_type":"markdown","id":"089b37d6","metadata":{"papermill":{"duration":0.004264,"end_time":"2023-03-11T13:38:59.016837","exception":false,"start_time":"2023-03-11T13:38:59.012573","status":"completed"},"tags":[]},"source":[" <a id=\"2\"></a>\n","<h1 style='background:#FF9F00;border:0; color:black;\n","    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n","    transform: rotateX(10deg);\n","    '><center style='color: #3E3D53;'>Data Preparation</center></h1>\n","    \n","# Data Prepartion"]},{"cell_type":"code","execution_count":2,"id":"1646141f","metadata":{"execution":{"iopub.execute_input":"2023-03-11T13:38:59.032455Z","iopub.status.busy":"2023-03-11T13:38:59.031752Z","iopub.status.idle":"2023-03-11T13:38:59.047805Z","shell.execute_reply":"2023-03-11T13:38:59.046823Z"},"papermill":{"duration":0.027929,"end_time":"2023-03-11T13:38:59.050771","exception":false,"start_time":"2023-03-11T13:38:59.022842","status":"completed"},"tags":[]},"outputs":[],"source":["class DataGenerator:\n","    \"\"\"\n","    A class for reading and preprocessing text data.\n","    \"\"\"\n","\n","    def __init__(self, path: str, sequence_length: int):\n","        \"\"\"\n","        Initializes a DataReader object with the path to a text file and the desired sequence length.\n","\n","        Args:\n","            path (str): The path to the text file.\n","            sequence_length (int): The length of the sequences that will be fed to the self.\n","        \"\"\"\n","        with open(path) as f:\n","            # Read the contents of the file\n","            self.data = f.read()\n","\n","        # Find all unique characters in the text\n","        chars = list(set(self.data))\n","\n","        # Create dictionaries to map characters to indices and vice versa\n","        self.char_to_idx = {ch: i for (i, ch) in enumerate(chars)}\n","        self.idx_to_char = {i: ch for (i, ch) in enumerate(chars)}\n","\n","        # Store the size of the text data and the size of the vocabulary\n","        self.data_size = len(self.data)\n","        self.vocab_size = len(chars)\n","\n","        # Initialize the pointer that will be used to generate sequences\n","        self.pointer = 0\n","\n","        # Store the desired sequence length\n","        self.sequence_length = sequence_length\n","\n","\n","    def next_batch(self):\n","        \"\"\"\n","        Generates a batch of input and target sequences.\n","\n","        Returns:\n","            inputs_one_hot (np.ndarray): A numpy array with shape `(batch_size, vocab_size)` where each row is a one-hot encoded representation of a character in the input sequence.\n","            targets (list): A list of integers that correspond to the indices of the characters in the target sequence, which is the same as the input sequence shifted by one position to the right.\n","        \"\"\"\n","        input_start = self.pointer\n","        input_end = self.pointer + self.sequence_length\n","\n","        # Get the input sequence as a list of integers\n","        inputs = [self.char_to_idx[ch] for ch in self.data[input_start:input_end]]\n","\n","        # One-hot encode the input sequence\n","        inputs_one_hot = np.zeros((len(inputs), self.vocab_size))\n","        inputs_one_hot[np.arange(len(inputs)), inputs] = 1\n","\n","        # Get the target sequence as a list of integers\n","        targets = [self.char_to_idx[ch] for ch in self.data[input_start + 1:input_end + 1]]\n","\n","        # Update the pointer\n","        self.pointer += self.sequence_length\n","\n","        # Reset the pointer if the next batch would exceed the length of the text data\n","        if self.pointer + self.sequence_length + 1 >= self.data_size:\n","            self.pointer = 0\n","\n","        return inputs_one_hot, targets"]},{"cell_type":"markdown","id":"7327e7ef","metadata":{"papermill":{"duration":0.007062,"end_time":"2023-03-11T13:38:59.064487","exception":false,"start_time":"2023-03-11T13:38:59.057425","status":"completed"},"tags":[]},"source":["<a id=\"3\"></a>\n","<h1 style='background:#FF9F00;border:0; color:black;\n","    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n","    transform: rotateX(10deg);\n","    '><center style='color: #3E3D53;'>LSTM Implementation</center></h1>\n","\n","\n","# LSTM Implementation\n"]},{"cell_type":"markdown","id":"88b49851","metadata":{"papermill":{"duration":0.004183,"end_time":"2023-03-11T13:38:59.075565","exception":false,"start_time":"2023-03-11T13:38:59.071382","status":"completed"},"tags":[]},"source":["## Forward Propagation\n","\n","**The forward pass is a step in training a simple LSTM (Long Short-Term Memory) model. During the forward pass, the model takes an input sequence X and performs a series of computations to generate a sequence of output probability vectors y_pred, which represent the predicted probability distribution over the possible output classes for each time step in the input sequence.**\n","\n","**The LSTM model has a memory cell that stores information from previous time steps, which is updated based on the input sequence and a set of learned parameters. During the forward pass, the LSTM computes a set of vectors representing the forget gate, input gate, candidate cell state, and output gate for each time step in the input sequence.**\n","\n","**These vectors are used to update the cell state and hidden state for each time step, which are then used to generate the output probability vectors. The function returns the input sequence, the cell state for each time step, the forget gate, input gate, output gate, candidate cell state, hidden state, and output probability vector for each time step.**\n","\n","**Here are the formulas used in the code:**\n","\n","**Concatenation of the input and hidden state:**\n","$$\\text{concat} = \\begin{bmatrix} a_{t-1} \\ x_t \\end{bmatrix}$$\n","\n","**Forget gate:**\n","**$$f_t = \\sigma(W_f \\text{concat} + b_f)$$**\n","\n","**Input gate:**\n","**$$i_t = \\sigma(W_i \\text{concat} + b_i)$$**\n","\n","**Candidate cell state:**\n","**$$\\tilde{c}_t = \\tanh(W_c \\text{concat} + b_c)$$**\n","\n","**Cell state:**\n","**$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$**\n","\n","**Output gate:**\n","**$$o_t = \\sigma(W_o \\text{concat} + b_o)$$**\n","\n","**Hidden state:**\n","**$$a_t = o_t \\odot \\tanh(c_t)$$**\n","\n","**Output probability vector:**\n","**$$\\hat{y}_t = \\text{softmax}(W_y a_t + b_y)$$**\n","\n","**Where:**\n","\n","**$x_t$ is the input vector at time $t$.**\n","\n","**$a_{t-1}$ is the hidden state vector at time $t-1$.**\n","\n","**$f_t$, $i_t$, $o_t$ are the forget, input, and output gates at time $t$, respectively.**\n","\n","**$c_t$ is the cell state vector at time $t$.**\n","\n","**$\\tilde{c}_t$ is the candidate cell state at time $t$.**\n","\n","$\\hat{y}_t$ is the output probability vector at time $t$.\n","\n","**$\\odot$ is the element-wise multiplication operator.**\n","\n","**$\\sigma$ is the sigmoid activation function.**\n","\n","**$\\text{tanh}$ is the hyperbolic tangent activation function.**\n","\n","**$W_f, W_i, W_c, W_o, W_y$ are the weight matrices for the forget, input, candidate, output, and output layers, respectively.**\n","\n","**$b_f, b_i, b_c, b_o, b_y$ are the bias vectors for the forget, input, candidate, output, and output layers, respectively.**\n","****\n","## Backpropagation\n","**The backpropagation step in training a simple LSTM model involves computing the gradients of the loss with respect to the model parameters, which are used to update the parameters in the opposite direction of the gradient to minimize the loss function.**\n","\n","**In this particular implementation of the LSTM model, the backpropagation step is implemented using the standard backpropagation through time (BPTT) algorithm. The algorithm iteratively computes the gradients of the loss with respect to each parameter in the model by propagating the gradients backwards through time from the output sequence to the input sequence.**\n","\n","**Starting from the final time step, the gradients of the loss with respect to the output probability vectors are computed using the cross-entropy loss function. The gradients are then propagated backwards through time by computing the gradients of the loss with respect to the hidden state and cell state for each time step.**\n","\n","**These gradients are used to compute the gradients of the loss with respect to the output gate, candidate cell state, input gate, and forget gate vectors for each time step, which are in turn used to compute the gradients of the loss with respect to the weight matrices and bias vectors for each gate and the output layer.**\n","\n","**The gradients are then accumulated across all time steps and used to update the model parameters using an optimization algorithm, such as stochastic gradient descent (SGD). This process is repeated for each batch of input sequences during training, until the model converges to a set of optimal parameters that minimize the loss function.**\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W_y}} = \\sum_{t=1}^T \\mathbf{a}_t \\cdot (\\mathbf{y}_t - \\mathbf{t}_t)^\\top$$\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b_y}} = \\sum_{t=1}^T (\\mathbf{y}_t - \\mathbf{t}_t)$$\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_t} = \\mathbf{W_y}^\\top \\cdot (\\mathbf{y}_t - \\mathbf{t}t) + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}{t+1}} \\cdot \\mathbf{W_f}^\\top \\cdot \\mathbf{f}_t + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\cdot \\mathbf{W_c}^\\top \\cdot \\mathbf{i}_t + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{o}_t} \\cdot \\mathbf{W_o}^\\top \\cdot \\mathbf{o}_t$$\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}{t+1}} \\cdot \\mathbf{f}_t + \\mathbf{i}_t \\cdot \\mathbf{c}_t \\cdot (1 - \\mathbf{c}_t) \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_t}$$\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{i}_t} = \\mathbf{c}_t \\cdot (1 - \\mathbf{i}_t) \\cdot \\mathbf{i}_t \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t}$$\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{f}t} = \\mathbf{c}{t-1} \\cdot (1 - \\mathbf{f}_t) \\cdot \\mathbf{f}_t \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t}$$\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{o}_t} = \\mathbf{a}_t \\cdot (1 - \\mathbf{o}_t) \\cdot \\mathbf{o}_t \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t}$$\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_{t-1}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\cdot \\mathbf{f}_t$$\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W_c}} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\cdot \\mathbf{i}_t \\cdot (1 - \\mathbf{c}_t^2) \\cdot \\mathbf{concat}_t^\\top$$\n","****\n","## Loss:\n","\n","**The cross-entropy loss between the predicted probabilities y_pred and the true targets y_true at a single time step $t$ is:**\n","\n","**$$H(y_{true,t}, y_{pred,t}) = -\\sum_i y_{true,t,i} \\log(y_{pred,t,i})$$**\n","\n","**where $y_{pred,t}$ is the predicted probability distribution at time step $t$, $y_{true,t}$ is the true probability distribution at time step $t$ (i.e., a one-hot encoded vector representing the true target), and $i$ ranges over the vocabulary size.**\n","\n","**The total loss is then computed as the sum of the cross-entropy losses over all time steps:**\n","\n","**$$L = \\sum_{t=1}^{T} H(y_{true,t}, y_{pred,t})$$**\n","\n","**where $T$ is the sequence length.**\n"," \n","****\n","## Optimization\n","\n","\n","**The AdamW optimizer is an extension of the Adam optimizer that incorporates weight decay directly into the update rule, rather than treating it as a separate regularization term. This has been shown to improve the generalization performance of deep neural networks.**\n","\n","**To use the AdamW optimizer in practice, we first initialize the parameters $W_f$, $b_f$, $W_i$, $b_i$, $W_c$, $b_c$, $W_o$, and $b_o$ with small random values. Then, for each training iteration, we compute the gradients of the loss function with respect to the parameters using backpropagation. These gradients are then used to update the parameters using the AdamW update rules described above. This process is repeated for a fixed number of iterations or until convergence is achieved.**\n","\n","**In summary, the AdamW optimizer is a powerful optimization algorithm for training deep neural networks. By incorporating weight decay directly into the update rule, it can improve the generalization performance of the network and reduce the risk of overfitting.**\n","\n","**For the AdamW update of parameter Wf:**\n","\n","$m_{Wf} \\leftarrow \\beta_1 m_{Wf} + (1 - \\beta_1) \\nabla_{Wf} J$\n","\n","$v_{Wf} \\leftarrow \\beta_2 v_{Wf} + (1 - \\beta_2) (\\nabla_{Wf} J)^2$\n","\n","$\\hat{m}{Wf} = \\frac{m{Wf}}{1 - \\beta_1}$\n","\n","$\\hat{v}{Wf} = \\frac{v{Wf}}{1 - \\beta_2}$\n","\n","$W_f \\leftarrow W_f - \\eta \\frac{\\hat{m}{Wf}}{\\sqrt{\\hat{v}{Wf}} + \\epsilon} - \\eta \\lambda_2 W_f$\n","\n","**For the AdamW update of parameter bf:**\n","\n","$m_{bf} \\leftarrow \\beta_1 m_{bf} + (1 - \\beta_1) \\nabla_{bf} J$\n","\n","$v_{bf} \\leftarrow \\beta_2 v_{bf} + (1 - \\beta_2) (\\nabla_{bf} J)^2$\n","\n","$\\hat{m}{bf} = \\frac{m{bf}}{1 - \\beta_1}$\n","\n","$\\hat{v}{bf} = \\frac{v{bf}}{1 - \\beta_2}$\n","\n","$b_f \\leftarrow b_f - \\eta \\frac{\\hat{m}{bf}}{\\sqrt{\\hat{v}{bf}} + \\epsilon} - \\eta \\lambda_2 b_f$\n","\n","**For the AdamW update of parameter Wi:**\n","\n","$m_{Wi} \\leftarrow \\beta_1 m_{Wi} + (1 - \\beta_1) \\nabla_{Wi} J$\n","\n","$v_{Wi} \\leftarrow \\beta_2 v_{Wi} + (1 - \\beta_2) (\\nabla_{Wi} J)^2$\n","\n","$\\hat{m}{Wi} = \\frac{m{Wi}}{1 - \\beta_1}$\n","\n","$\\hat{v}{Wi} = \\frac{v{Wi}}{1 - \\beta_2}$\n","\n","$W_i \\leftarrow W_i - \\eta \\frac{\\hat{m}{Wi}}{\\sqrt{\\hat{v}{Wi}} + \\epsilon} - \\eta \\lambda_2 W_i$\n","\n","**For the AdamW update of parameter bi:**\n","\n","$m_{bi} \\leftarrow \\beta_1 m_{bi} + (1 - \\beta_1) \\nabla_{bi} J$\n","\n","$v_{bi} \\leftarrow \\beta_2 v_{bi} + (1 - \\beta_2) (\\nabla_{bi} J)^2$\n","\n","$\\hat{m}{bi} = \\frac{m{bi}}{1 - \\beta_1}$\n","\n","$\\hat{v}{bi} = \\frac{v{bi}}{1 - \\beta_2}$\n","\n","$b_i \\leftarrow b_i - \\eta \\frac{\\hat{m}{bi}}{\\sqrt{\\hat{v}{bi}} + \\epsilon} - \\eta \\lambda_2 b_i$\n","\n","**For the AdamW update of parameter Wc:**\n","\n","$m_{Wc} \\leftarrow \\beta_1 m_{Wc} + (1 - \\beta_1) \\nabla_{Wc} J$\n","\n","$v_{Wc} \\leftarrow \\beta_2 v_{Wc} + (1 - \\beta2) (\\nabla{Wc} J)^2$\n","\n","$\\hat{m}{Wc} = \\frac{m{Wc}}{1 - \\beta_1}$\n","\n","$\\hat{v}{Wc} = \\frac{v{Wc}}{1 - \\beta_2}$\n","\n","$W_c \\leftarrow W_c - \\eta \\frac{\\hat{m}{Wc}}{\\sqrt{\\hat{v}{Wc}} + \\epsilon} - \\eta \\lambda_2 W_c$\n","\n","**For the AdamW update of parameter bc:**\n","\n","$m_{bc} \\leftarrow \\beta_1 m_{bc} + (1 - \\beta_1) \\nabla_{bc} J$\n","\n","$v_{bc} \\leftarrow \\beta_2 v_{bc} + (1 - \\beta_2) (\\nabla_{bc} J)^2$\n","\n","$\\hat{m}{bc} = \\frac{m{bc}}{1 - \\beta_1}$\n","\n","$\\hat{v}{bc} = \\frac{v{bc}}{1 - \\beta_2}$\n","\n","$b_c \\leftarrow b_c - \\eta \\frac{\\hat{m}{bc}}{\\sqrt{\\hat{v}{bc}} + \\epsilon} - \\eta \\lambda_2 b_c$\n","\n","**For the AdamW update of parameter Wo:**\n","\n","$m_{Wo} \\leftarrow \\beta_1 m_{Wo} + (1 - \\beta_1) \\nabla_{Wo} J$\n","\n","$v_{Wo} \\leftarrow \\beta_2 v_{Wo} + (1 - \\beta_2) (\\nabla_{Wo} J)^2$\n","\n","$\\hat{m}{Wo} = \\frac{m{Wo}}{1 - \\beta_1}$\n","\n","$\\hat{v}{Wo} = \\frac{v{Wo}}{1 - \\beta_2}$\n","\n","$W_o \\leftarrow W_o - \\eta \\frac{\\hat{m}{Wo}}{\\sqrt{\\hat{v}{Wo}} + \\epsilon} - \\eta \\lambda_2 W_o$\n","\n","**For the AdamW update of parameter bo:**\n","\n","$m_{bo} \\leftarrow \\beta_1 m_{bo} + (1 - \\beta_1) \\nabla_{bo} J$\n","\n","$v_{bo} \\leftarrow \\beta_2 v_{bo} + (1 - \\beta_2) (\\nabla_{bo} J)^2$\n","\n","$\\hat{m}{bo} = \\frac{m{bo}}{1 - \\beta_1}$\n","\n","$\\hat{v}{bo} = \\frac{v{bo}}{1 - \\beta_2}$\n","\n","**$b_o \\leftarrow b_o - \\eta \\frac{\\hat{m}{bo}}{\\sqrt{\\hat{v}{bo}} + \\epsilon} - \\eta \\lambda_2 b_o$**\n","\n","**In the above formulas, $\\nabla_{Wf} J$, $\\nabla_{bf} J$, $\\nabla_{Wi} J$, $\\nabla_{bi} J$, $\\nabla_{Wc} J$, $\\nabla_{bc} J$, $\\nabla_{Wo} J$, and $\\nabla_{bo} J$ are the gradients of the loss function J with respect to the parameters $W_f$, $b_f$, $W_i$, $b_i$, $W_c$, $b_c$, $W_o$, and $b_o$ respectively. $\\beta_1$ and $\\beta_2$ are hyperparameters controlling the exponential decay rates of the moving averages. $\\eta$ is the learning rate, $\\lambda_2$ is the weight decay hyperparameter, and $\\epsilon$ is a small constant to avoid division by zero.**\n","\n","\n","\n","****\n","## Train\n","\n","**The train method trains the LSTM on a dataset using backpropagation through time. The method takes an instance of DataReader containing the training data as input. The method initializes a hidden state vector a_prev at the beginning of each sequence to zero.**\n","\n","**It then iterates until the smooth loss is less than a threshold value. During each iteration, it retrieves a batch of inputs and targets from the data generator.**\n","\n","**The LSTM then performs a forward pass on the input sequence and computes the output probabilities. The backward pass is performed using the targets and output probabilities to calculate the gradients of the parameters of the network.**\n","\n","**The AdamW algorithm is used to update the weights of the network. The method then calculates and updates the loss using the updated weights. The previous hidden state is updated for the next batch.**\n","\n","**The method prints the progress every 10000 iterations by generating a sample of text using the sample method and printing the loss.**\n","\n","**The train method can be summarized by the following steps:**\n","\n","**$1.$ Initialize $a_{prev}$ to zero at the beginning of each sequence.**\n","\n","**$2.$ Retrieve a batch of inputs and targets from the data generator.**\n","\n","**$3.$ Perform a forward pass on the input sequence and compute the output probabilities.**\n","\n","**$4.$ Perform a backward pass using the targets and output probabilities to calculate the gradients of the parameters of the network.**\n","\n","**$5.$ Use the AdamW algorithm to update the weights of the network.**\n","\n","**$6.$ Calculate and update the loss using the updated weights.**\n","\n","**$7.$ Update the previous hidden state for the next batch.**\n","\n","**$8.$ Print progress every 1000 iterations by generating a sample of text using the sample method and printing the loss.**\n","\n","**$9.$ Repeat steps $2$-$8$ until the smooth loss is less than the threshold value.**"]},{"cell_type":"code","execution_count":3,"id":"24044efc","metadata":{"execution":{"iopub.execute_input":"2023-03-11T13:38:59.088298Z","iopub.status.busy":"2023-03-11T13:38:59.087914Z","iopub.status.idle":"2023-03-11T13:38:59.170801Z","shell.execute_reply":"2023-03-11T13:38:59.169462Z"},"papermill":{"duration":0.093392,"end_time":"2023-03-11T13:38:59.173649","exception":false,"start_time":"2023-03-11T13:38:59.080257","status":"completed"},"tags":[]},"outputs":[],"source":[" class LSTM:\n","    \"\"\"\n","    A class used to represent a Recurrent Neural Network (LSTM).\n","\n","    Attributes\n","    ----------\n","    hidden_size : int\n","        The number of hidden units in the LSTM.\n","    vocab_size : int\n","        The size of the vocabulary used by the LSTM.\n","    sequence_length : int\n","        The length of the input sequences fed to the LSTM.\n","    self.learning_rate : float\n","        The learning rate used during training.\n","\n","    Methods\n","    -------\n","    __init__(hidden_size, vocab_size, sequence_length, self.learning_rate)\n","        Initializes an instance of the LSTM class.\n","    \"\"\"\n","\n","    def __init__(self, hidden_size, vocab_size, sequence_length, learning_rate):\n","        \"\"\"\n","        Initializes an instance of the LSTM class.\n","\n","        Parameters\n","        ----------\n","        hidden_size : int\n","            The number of hidden units in the LSTM.\n","        vocab_size : int\n","            The size of the vocabulary used by the LSTM.\n","        sequence_length : int\n","            The length of the input sequences fed to the LSTM.\n","        learning_rate : float\n","            The learning rate used during training.\n","        \"\"\"\n","        # hyper parameters\n","        self.mby = None\n","        self.hidden_size = hidden_size\n","        self.vocab_size = vocab_size\n","        self.sequence_length = sequence_length\n","        self.learning_rate = learning_rate\n","        \n","        # model parameters\n","        self.Wf = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n","                                    (hidden_size, hidden_size + vocab_size))\n","        self.bf = np.zeros((hidden_size, 1))\n","        \n","        self.Wi = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n","                                    (hidden_size, hidden_size + vocab_size))\n","        self.bi = np.zeros((hidden_size, 1))\n","\n","        self.Wc = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n","                                    (hidden_size, hidden_size + vocab_size))\n","        self.bc = np.zeros((hidden_size, 1))\n","            \n","        self.Wo = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n","                                    (hidden_size, hidden_size + vocab_size))\n","        self.bo = np.zeros((hidden_size, 1))\n","        \n","        self.Wy = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n","                                    (vocab_size, hidden_size))\n","        self.by = np.zeros((vocab_size, 1))\n","\n","        # initialize parameters for adamw optimizer\n","        self.mWf = np.zeros_like(self.Wf)\n","        self.vWf = np.zeros_like(self.Wf)\n","        self.mWi = np.zeros_like(self.Wi)\n","        self.vWi = np.zeros_like(self.Wi)\n","        self.mWc = np.zeros_like(self.Wc)\n","        self.vWc = np.zeros_like(self.Wc)\n","        self.mWo = np.zeros_like(self.Wo)\n","        self.vWo = np.zeros_like(self.Wo)\n","        self.mWy = np.zeros_like(self.Wy)\n","        self.vWy = np.zeros_like(self.Wy)\n","        self.mbf = np.zeros_like(self.bf)\n","        self.vbf = np.zeros_like(self.bf)\n","        self.mbi = np.zeros_like(self.bi)\n","        self.vbi = np.zeros_like(self.bi)\n","        self.mbc = np.zeros_like(self.bc)\n","        self.vbc = np.zeros_like(self.bc)\n","        self.mbo = np.zeros_like(self.bo)\n","        self.vbo = np.zeros_like(self.bo)\n","        self.mby = np.zeros_like(self.by)\n","        self.vby = np.zeros_like(self.by)\n","\n","    def sigmoid(self, x):\n","        \"\"\"\n","        Computes the sigmoid activation function for a given input array.\n","\n","        Parameters:\n","            x (ndarray): Input array.\n","\n","        Returns:\n","            ndarray: Array of the same shape as `x`, containing the sigmoid activation values.\n","        \"\"\"\n","        return 1 / (1 + np.exp(-x))\n","\n","    def softmax(self, x):\n","        \"\"\"\n","        Computes the softmax activation function for a given input array.\n","\n","        Parameters:\n","            x (ndarray): Input array.\n","\n","        Returns:\n","            ndarray: Array of the same shape as `x`, containing the softmax activation values.\n","        \"\"\"\n","        # shift the input to prevent overflow when computing the exponentials\n","        x = x - np.max(x)\n","        # compute the exponentials of the shifted input\n","        p = np.exp(x)\n","        # normalize the exponentials by dividing by their sum\n","        return p / np.sum(p)\n","\n","    def loss(self, y_preds, targets):\n","        \"\"\"\n","        Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets.\n","\n","        Parameters:\n","            y_preds (ndarray): Array of shape (sequence_length, vocab_size) containing the predicted probabilities for each time step.\n","            targets (ndarray): Array of shape (sequence_length, 1) containing the true targets for each time step.\n","\n","        Returns:\n","            float: Cross-entropy loss.\n","        \"\"\"\n","        # calculate cross-entropy loss\n","        return sum(-np.log(y_preds[t][targets[t], 0]) for t in range(self.sequence_length))\n","    \n","    \n","    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n","        \"\"\"\n","        Updates the LSTM's parameters using the AdamW optimization algorithm.\n","        \"\"\"\n","        # AdamW update for Wf\n","        self.mWf = beta1 * self.mWf + (1 - beta1) * self.dWf\n","        self.vWf = beta2 * self.vWf + (1 - beta2) * np.square(self.dWf)\n","        m_hat = self.mWf / (1 - beta1)\n","        v_hat = self.vWf / (1 - beta2)\n","        self.Wf -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wf)\n","\n","        # AdamW update for bf\n","        self.mbf = beta1 * self.mbf + (1 - beta1) * self.dbf\n","        self.vbf = beta2 * self.vbf + (1 - beta2) * np.square(self.dbf)\n","        m_hat = self.mbf / (1 - beta1)\n","        v_hat = self.vbf / (1 - beta2)\n","        self.bf -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bf)\n","\n","        # AdamW update for Wi\n","        self.mWi = beta1 * self.mWi + (1 - beta1) * self.dWi\n","        self.vWi = beta2 * self.vWi + (1 - beta2) * np.square(self.dWi)\n","        m_hat = self.mWi / (1 - beta1)\n","        v_hat = self.vWi / (1 - beta2)\n","        self.Wi -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wi)\n","\n","        # AdamW update for bi\n","        self.mbi = beta1 * self.mbi + (1 - beta1) * self.dbi\n","        self.vbi = beta2 * self.vbi + (1 - beta2) * np.square(self.dbi)\n","        m_hat = self.mbi / (1 - beta1)\n","        v_hat = self.vbi / (1 - beta2)\n","        self.bi -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bi)\n","\n","        # AdamW update for Wc\n","        self.mWc = beta1 * self.mWc + (1 - beta1) * self.dWc\n","        self.vWc = beta2 * self.vWc + (1 - beta2) * np.square(self.dWc)\n","        m_hat = self.mWc / (1 - beta1)\n","        v_hat = self.vWc / (1 - beta2)\n","        self.Wc -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wc)\n","\n","        # AdamW update for bc\n","        self.mbc = beta1 * self.mbc + (1 - beta1) * self.dbc\n","        self.vbc = beta2 * self.vbc + (1 - beta2) * np.square(self.dbc)\n","        m_hat = self.mbc / (1 - beta1)\n","        v_hat = self.vbc / (1 - beta2)\n","        self.bc -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bc)\n","\n","        # AdamW update for Wy\n","        self.mWy = beta1 * self.mWy + (1 - beta1) * self.dWy\n","        self.vWy = beta2 * self.vWy + (1 - beta2) * np.square(self.dWy)\n","        m_hat = self.mWy / (1 - beta1)\n","        v_hat = self.vWy / (1 - beta2)\n","        self.Wy -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wy)\n","        # AdamW update for by\n","        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n","        self.vby = beta2 * self.vby + (1 - beta2) * np.square(self.dby)\n","        m_hat = self.mby / (1 - beta1)\n","        v_hat = self.vby / (1 - beta2)\n","        self.by -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.by)\n","\n","\n","    def forward(self, X, c_prev, a_prev):\n","        \"\"\"\n","        Performs forward propagation for a simple LSTM model.\n","\n","        Args:\n","            X (numpy array): Input sequence, shape (sequence_length, input_size)\n","            c_prev (numpy array): Previous cell state, shape (hidden_size, 1)\n","            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n","\n","        Returns:\n","            X (numpy array): Input sequence, shape (sequence_length, input_size)\n","            c (dictionary): Cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n","            f (dictionary): Forget gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n","            i (dictionary): Input gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n","            o (dictionary): Output gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n","            cc (dictionary): Candidate cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n","            a (dictionary): Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n","            y_pred (dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (output_size, 1)\n","        \"\"\"\n","        # initialize dictionaries for backpropagation \n","        c, f, i, o, cc, a, y_pred = {}, {}, {}, {}, {}, {}, {}\n","        c[-1] = np.copy(c_prev)  # store the initial cell state in the dictionary\n","        a[-1] = np.copy(a_prev)  # store the initial hidden state in the dictionary\n","\n","        # iterate over each time step in the input sequence\n","        for t in range(X.shape[0]):\n","            # concatenate the input and hidden state\n","            xt = X[t, :].reshape(-1, 1)\n","            concat = np.vstack((a[t - 1], xt))\n","\n","            # compute the forget gate\n","            f[t] = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n","\n","            # compute the input gate\n","            i[t] = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n","\n","            # compute the candidate cell state\n","            cc[t] = np.tanh(np.dot(self.Wc, concat) + self.bc)\n","\n","            # compute the cell state\n","            c[t] = f[t] * c[t - 1] + i[t] * cc[t]\n","\n","            # compute the output gate\n","            o[t] = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n","\n","            # compute the hidden state\n","            a[t] = o[t] * np.tanh(c[t])\n","\n","            # compute the output probability vector\n","            y_pred[t] = self.softmax(np.dot(self.Wy, a[t]) + self.by)\n","\n","        # return the output probability vectors, cell state, hidden state and gate vectors\n","        return X, y_pred, c, f, i, o, cc, a \n","    \n","    \n","    def backward(self, X, targets, y_pred, c_prev, a_prev, c, f, i, o, cc, a):\n","        \"\"\"\n","        Performs backward propagation through time for an LSTM network.\n","\n","        Args:\n","        - X: input data for each time step, with shape (sequence_length, input_size)\n","        - targets: target outputs for each time step, with shape (sequence_length, output_size)\n","        - y_pred: predicted outputs for each time step, with shape (sequence_length, output_size)\n","        - c_prev: previous cell state, with shape (hidden_size, 1)\n","        - a_prev: previous hidden state, with shape (hidden_size, 1)\n","        - c: cell state for each time step, with shape (sequence_length, hidden_size)\n","        - f: forget gate output for each time step, with shape (sequence_length, hidden_size)\n","        - i: input gate output for each time step, with shape (sequence_length, hidden_size)\n","        - o: output gate output for each time step, with shape (sequence_length, hidden_size)\n","        - cc: candidate cell state for each time step, with shape (sequence_length, hidden_size)\n","        - a: hidden state output for each time step, with shape (sequence_length, hidden_size)\n","        Returns:\n","            None\n","        \"\"\"\n","        \n","        # initialize gradients for each parameter\n","        self.dWf, self.dWi, self.dWc, self.dWo, self.dWy = np.zeros_like(self.Wf), np.zeros_like(self.Wi), np.zeros_like(self.Wc), np.zeros_like(self.Wo), np.zeros_like(self.Wy)\n","        self.dbf, self.dbi, self.dbc, self.dbo, self.dby = np.zeros_like(self.bf), np.zeros_like(self.bi), np.zeros_like(self.bc), np.zeros_like(self.bo), np.zeros_like(self.by)\n","        dc_next = np.zeros_like(c_prev)\n","        da_next = np.zeros_like(a_prev)\n","\n","        # iterate backwards through time steps\n","        for t in reversed(range(X.shape[0])):\n","            # compute the gradient of the output probability vector\n","            dy = np.copy(y_pred[t])\n","            dy[targets[t]] -= 1\n","\n","            # compute the gradient of the output layer weights and biases\n","            self.dWy += np.dot(dy, a[t].T)\n","            self.dby += dy\n","\n","            # compute the gradient of the hidden state\n","            da = np.dot(self.Wy.T, dy) + da_next\n","            dc = dc_next + (1 - np.tanh(c[t])**2) * o[t] * da\n","            \n","            # compute the gradient of the output gate\n","            xt = X[t, :].reshape(-1, 1)\n","            concat = np.vstack((a[t - 1], xt))\n","            do = o[t] * (1 - o[t]) * np.tanh(c[t]) * da\n","            self.dWo += np.dot(do, concat.T)\n","            self.dbo += do\n","\n","            # compute the gradient of the candidate cell state\n","            dcc = dc * i[t] * (1 - np.tanh(cc[t])**2)\n","            self.dWc += np.dot(dcc, concat.T)\n","            self.dbc += dcc\n","\n","            # compute the gradient of the input gate\n","            di = i[t] * (1 - i[t]) * cc[t] * dc\n","            self.dWi += np.dot(di, concat.T)\n","            self.dbi += di\n","\n","            # compute the gradient of the forget gate\n","            df = f[t] * (1 - f[t]) * c[t - 1] * dc\n","            self.dWf += np.dot(df, concat.T)\n","            self.dbf += df\n","\n","            # compute the gradient of the input to the current hidden state and cell state\n","            da_next = np.dot(self.Wf[:, :self.hidden_size].T, df)\\\n","            + np.dot(self.Wi[:, :self.hidden_size].T, di)\\\n","            + np.dot(self.Wc[:, :self.hidden_size].T, dcc)\\\n","            + np.dot(self.Wo[:, :self.hidden_size].T, do)\n","            dc_next = dc * f[t]\n","\n","        # clip gradients to avoid exploding gradients\n","        for grad in [self.dWf, self.dWi, self.dWc, self.dWo, self.dWy, self.dbf, self.dbi, self.dbc, self.dbo, self.dby]:\n","            np.clip(grad, -1, 1, out=grad)\n","\n","\n","    def train(self, data_generator):\n","        \"\"\"\n","        Train the LSTM on a dataset using backpropagation through time.\n","\n","        Args:\n","            data_generator: An instance of DataGenerator containing the training data.\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        iter_num = 0\n","        # stopping criterion for training\n","        threshold = 46\n","        smooth_loss = -np.log(1.0 / data_generator.vocab_size) * self.sequence_length  # initialize loss\n","        while (smooth_loss > threshold):\n","            # initialize hidden state at the beginning of each sequence\n","            if data_generator.pointer == 0:\n","                c_prev = np.zeros((self.hidden_size, 1))\n","                a_prev = np.zeros((self.hidden_size, 1))\n","\n","            # get a batch of inputs and targets\n","            inputs, targets = data_generator.next_batch()\n","\n","            # forward pass\n","            X, y_pred, c, f, i, o, cc, a   = self.forward(inputs, c_prev, a_prev)\n","        \n","            # backward pass\n","            self.backward( X, targets, y_pred, c_prev, a_prev, c, f, i, o, cc, a)\n","\n","            # calculate and update loss\n","            loss = self.loss(y_pred, targets)\n","            self.adamw()\n","            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n","            # update previous hidden state for the next batch\n","            a_prev = a[self.sequence_length - 1]\n","            c_prev = c[self.sequence_length - 1]\n","            # print progress every 1000 iterations\n","            if iter_num % 1000 == 0:\n","                self.learning_rate *= 0.99\n","                sample_idx = self.sample(c_prev, a_prev, inputs[0, :], 200)\n","                print(''.join(data_generator.idx_to_char[idx] for idx in sample_idx))\n","                print(\"\\n\\niter :%d, loss:%f\" % (iter_num, smooth_loss))\n","            iter_num += 1\n","    \n","            \n","    def sample(self, c_prev, a_prev, seed_idx, n):\n","        \"\"\"\n","        Sample a sequence of integers from the model.\n","\n","        Args:\n","            c_prev (numpy.ndarray): Previous cell state, a numpy array of shape (hidden_size, 1).\n","            a_prev (numpy.ndarray): Previous hidden state, a numpy array of shape (hidden_size, 1).\n","            seed_idx (numpy.ndarray): Seed letter from the first time step, a numpy array of shape (vocab_size, 1).\n","            n (int): Number of characters to generate.\n","\n","        Returns:\n","            list: A list of integers representing the generated sequence.\n","\n","        \"\"\"\n","        # initialize input and seed_idx\n","        x = np.zeros((self.vocab_size, 1))\n","        # convert one-hot encoding to integer index\n","        seed_idx = np.argmax(seed_idx, axis=-1)\n","\n","        # set the seed letter as the input for the first time step\n","        x[seed_idx] = 1\n","\n","        # generate sequence of characters\n","        idxes = []\n","        c = np.copy(c_prev)\n","        a = np.copy(a_prev)\n","        for t in range(n):\n","            # compute the hidden state and cell state\n","            concat = np.vstack((a, x))\n","            i = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n","            f = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n","            cc = np.tanh(np.dot(self.Wc, concat) + self.bc)\n","            c = f * c + i * cc\n","            o = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n","            a = o * np.tanh(c)\n","\n","            # compute the output probabilities\n","            y = self.softmax(np.dot(self.Wy, a) + self.by)\n","\n","            # sample the next character from the output probabilities\n","            idx = np.random.choice(range(self.vocab_size), p=y.ravel())\n","\n","            # set the input for the next time step\n","            x = np.zeros((self.vocab_size, 1))\n","            x[idx] = 1\n","\n","            # append the sampled character to the sequence\n","            idxes.append(idx)\n","\n","        # return the generated sequence\n","        return idxes\n","\n","\n","    def predict(self, data_generator, start, n):\n","        \"\"\"\n","        Generate a sequence of n characters using the trained LSTM model, starting from the given start sequence.\n","\n","        Args:\n","        - data_generator: an instance of DataGenerator\n","        - start: a string containing the start sequence\n","        - n: an integer indicating the length of the generated sequence\n","\n","        Returns:\n","        - txt: a string containing the generated sequence\n","        \"\"\"\n","        # initialize input sequence\n","        x = np.zeros((self.vocab_size, 1))\n","        chars = [ch for ch in start]\n","        idxes = []\n","        for i in range(len(chars)):\n","            idx = data_generator.char_to_idx[chars[i]]\n","            x[idx] = 1\n","            idxes.append(idx)\n","        # initialize cell state and hidden state\n","        a = np.zeros((self.hidden_size, 1))\n","        c = np.zeros((self.hidden_size, 1))\n","            \n","        # generate new sequence of characters\n","        for t in range(n):\n","            # compute the hidden state and cell state\n","            concat = np.vstack((a, x))\n","            i = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n","            f = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n","            cc = np.tanh(np.dot(self.Wc, concat) + self.bc)\n","            c = f * c + i * cc\n","            o = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n","            a = o * np.tanh(c)\n","            # compute the output probabilities\n","            y_pred = self.softmax(np.dot(self.Wy, a) + self.by)\n","            # sample the next character from the output probabilities\n","            idx = np.random.choice(range(self.vocab_size), p=y_pred.ravel())\n","            x = np.zeros((self.vocab_size, 1))\n","            x[idx] = 1\n","            idxes.append(idx)\n","        \n","        txt = ''.join(data_generator.idx_to_char[i] for i in idxes)\n","        txt.replace('\\n',\"\")\n","        return txt\n"]},{"cell_type":"markdown","id":"802145d7","metadata":{"papermill":{"duration":0.003875,"end_time":"2023-03-11T13:38:59.181981","exception":false,"start_time":"2023-03-11T13:38:59.178106","status":"completed"},"tags":[]},"source":["<div class=\"alert alert-block alert-danger\">  \n","<b>Warning:</b> This takes a very long time to converge \n","</div>"]},{"cell_type":"code","execution_count":4,"id":"dea94d03","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-03-11T13:38:59.192291Z","iopub.status.busy":"2023-03-11T13:38:59.191639Z","iopub.status.idle":"2023-03-11T21:44:13.650763Z","shell.execute_reply":"2023-03-11T21:44:13.648852Z"},"papermill":{"duration":29114.469537,"end_time":"2023-03-11T21:44:13.655581","exception":false,"start_time":"2023-03-11T13:38:59.186044","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["guY&yBnqy$d3UeWmS;EM!&KiKGOE;U!Sid3K&BW3Mwk\n","ApIKY'WQsLnV\n","Qybwz',Cbfq:$FHixFYW$F DMDtG EHkpJ\n","Ydt,Zt:FymILHqQ'h'yXB,rvflD$xrLs$XuMSVFEubBY:XVsUJbxYDNnA,j&3yWbwCeHy w\n","UyYsJpVMNz!qwur:s!L\n","XMHsv$v-cMMj\n","WAO\n","\n","\n","iter :0, loss:116.882823\n","hine to ya sit to fhat iurn\n","svaln\n","n tnobg, nus sa cane thcae arcnld wals ant aml?\n","\n","op\n","II foplit andon Mhyimg uld a'r yirgis sasepd Wouh, Ih ool,\n","H than ars ar yfee goas iouternsongand be e als kens w \n","\n","\n","iter :1000, loss:93.119616\n","heve urt,\n","I wn, is thee woqt inkt aak thidd besoCis; Ao oule imanl se plorl celusf nptoun hall wouce houlT\n"," pop ar wounlud lorkith sid mnng dece bins rencore wosbhank coiceunge, ane ameet, dociwg he w\n","\n","\n","iter :2000, loss:76.101604\n","eectat of. ;atost enteithethath sirouts'ns o'edse oat bond yiu puskuather\n","Iald coarisiwing'myouRTLyourth! Bfiret yeu hof you te oul;\n","Oe rewoale as, ore blarb, his.\n","\n","o RiOmiIUS:\n","eeu. we alath ip,\n","NSong\n","\n","\n","iter :3000, loss:66.201321\n"," wien me har t hum siade ward co ginen is start anm sa hlf aice senRris afd kivh he hic\n","Sith halld ponese hes gt ont om oun mint mat on he the vows, ine ran that ware te wirit\n",".\n","\n","SidIs STercengorin be\n","\n","\n","iter :4000, loss:61.563293\n","gspyou.\n","Your hat with sorece cow wenw th senor. for thas thim sheyod;, befstund br\n","Bime now thavemyient Docann thiuid; he raceriva to hfored sp hishurres s'd-I\n","Ancell cow whst kith thour wo, uune rear\n","\n","\n","iter :5000, loss:59.303979\n","than thawe she uss,\n","As the Kicl wath I ate me petreTeund's sperle; Se wher whru, serusve;\n","Whine than melgrdetaithand go theee lomyuvito the katest,\n","And and bthete noe, buth mave pu. dagh thy che hage?\n","\n","\n","iter :6000, loss:58.473539\n","uMTreerer:\n","Not me williss withot uld macs\n","e'd wabls it homet, eot hive on erom.\n","\n","Card Roumururs:\n","What my wall stoud thim heshry, se lowl't dy stom\n","And with not make to veace the sullot?\n","Bot thet wely \n","\n","\n","iter :7000, loss:57.182984\n","okw afast, loct dt in my ins lath.\n","\n","BRUKNE ELIZABEIH:\n","Pock, oy oul, ald kripnas 'dverin; dade;\n","Thy would do mo to hale shes dean haels:\n","Mores on my gorgind greth my workcord:\n","God tha dwert, dold s yuk\n","\n","\n","iter :8000, loss:55.157419\n","h ngere in maalding ther and Lord\n","Dowel is you nreer harecheld.\n","Ar, corery t our wate The lite, the ingaatEm:\n","I war te the Goward you worl het hamk lake!\n","And the farinsy. 'y ghinc of henoddse-garcGhac\n","\n","\n","iter :9000, loss:53.797538\n","mest khave toutxthing veirt\n","EMwatnore on bithat thee Lorcerivene the kodc lonk:\n","O, I thle my merter that andtrqusigket reath:\n","\n","CUCHESS OF YORY:\n","Why brandirn daetesly grow. Yoor bractiend coneed wfor;\n","\n","\n","\n","iter :10000, loss:53.211613\n","vance daok jeas', far MadsunWhor-\n","Yoon, might Cacloulliend, the upine whild we the morme.\n","The hith, lomm liust in I wisl wist be Coml,\n","Bit hesse-sell'ds fromd if themen deonsas porte\n","And ssabe of my b\n","\n","\n","iter :11000, loss:51.920489\n","tham their draga;\n","Ang the mparliof whatt to ser wiknt will and dighted freclion\n","\n","BUCHINGHAM Hivond warbur;.\n","Whorgledr lertsing must what wnthed farth a curren frich.\n","\n","KHart Sruris MirsoNH:\n","Wmlce, de a\n","\n","\n","iter :12000, loss:52.936916\n","he kanat He sonetiy notse nese afe if sure'roue.\n","\n","SRNEH:\n","Bel Labke upss cimperte, grotuingF mesowshy load.\n","\n","DUEEN:\n","And thy nowhe anveed thay hourd mo\n","en she; hime,\n","Ae binks weer tissl of I klood,\n","Lere\n","\n","\n","iter :13000, loss:52.488089\n","nd tother onfue sand'l prowhrlght\n","Oir and to pars of to\n","Tus destifled ande freals;\n","Your be fleer senots,, of be chounse\n","As our manting hip his lay.\n","Whech enesting and this dagle, me teat,\n","Lerpevers, a\n","\n","\n","iter :14000, loss:51.631180\n","eanisa foreh you sejst sore veling!\n","Malg, my gow of to outher, and criest?\n","Thad, peifs uislfy, and thim heme in thay Boos\n","Hard fol our with him dladkery lidismy'st the sam.\n","\n","KING RICHARD II:\n","Weal him \n","\n","\n","iter :15000, loss:50.545426\n","ith.\n","\n","BUCzeA OF:\n","And uping a the farn Righer dabet,\n","Whe will'ser purieg of whth weinended, who are prowd lid,\n","No nother: prous ins is my here, where it eteed:\n","And thie, mother in ale and hithe sace ap\n","\n","\n","iter :16000, loss:50.081306\n","n is soienvertn.\n","\n","BULIOLTO:\n","Curtary,\n","That pertsenter, incheer feather!\n","\n","ARLEET:\n","Gou to my tothind nots, of lipe.\n","\n","SELELT:\n","Canteac!\n","'ting'd thet wandbe fire of she distlement nopse!\n","More grows hiven di\n","\n","\n","iter :17000, loss:51.783535\n","hall atyy\n","The sublabl of dauth nomed\n","At you soy may as your montreme, thes heas\n"," nor not shillevimest ir the herd.\n","O towd this reviries love thair sume onThe of\n","Locling mocherdased\n","I'sn proces chougwm\n","\n","\n","iter :18000, loss:50.844272\n","agher goot:\n","That hest fight the hadse your fair's thou rages and hell.\n","\n","JuIsO Poly,\n","Jole me sue,\n","Sang! Romeo woth must day?\n","I Joddot er, our cot tlen Callsou thou welk heve fealt.\n","\n","JULIET:\n","Stenle as, \n","\n","\n","iter :19000, loss:50.336434\n","n ipl:\n","Myselfat Do pord a arly, Tagray comer;!\n","Mising? welct. I will a weat him, in is it esparedpine wear?\n","\n","Frrait:\n","Sere you saked, gool: CAuls  'Tcl ther sup, a mam\n","It fetthings for gries? Mestling \n","\n","\n","iter :20000, loss:49.688655\n","cansloud and thy cleadan there one Yidn.\n","\n","KING HENRY VO:\n","With ters chinghing th mete thy partievet is\n","Brceat make my hather fills,\n","And not ant dim sensen thou sone seed you to Hend\n","Froa mo you day ent\n","\n","\n","iter :21000, loss:48.274789\n",".\n","O,-Lord a prownhols: him flome, for\n","Bewence young early were ach well;\n","The lorghy atuin thus a frounds faeme!\n","\n","WARWICH:\n","O, that roert how the hoy! Wol the like,\n","As to gut suh's angow's bethrow, if t\n","\n","\n","iter :22000, loss:48.239011\n"]}],"source":["sequence_length = 28\n","#read text from the \"input.txt\" file\n","data_generator = DataGenerator('/kaggle/input/shakespeare-text/text.txt', sequence_length)\n","lstm =  LSTM(hidden_size=1000, vocab_size=data_generator.vocab_size,sequence_length=sequence_length,learning_rate=1e-3)\n","lstm.train(data_generator)"]},{"cell_type":"code","execution_count":5,"id":"2d56935d","metadata":{"execution":{"iopub.execute_input":"2023-03-11T21:44:13.673273Z","iopub.status.busy":"2023-03-11T21:44:13.672545Z","iopub.status.idle":"2023-03-11T21:44:13.94619Z","shell.execute_reply":"2023-03-11T21:44:13.944994Z"},"papermill":{"duration":0.284775,"end_time":"2023-03-11T21:44:13.950055","exception":false,"start_time":"2023-03-11T21:44:13.66528","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["\"cen'dr'd hame.\\nFor stanks it lices, my lave a wropnth comfide\\nBut ligets to mery donate sosear;\\nWoy day leve then I confol my tount,\\nTo mast dead as ki\""]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["lstm.predict(data_generator, \"c\", 150)"]},{"cell_type":"markdown","id":"d9f826f4","metadata":{"papermill":{"duration":0.014934,"end_time":"2023-03-11T21:44:13.980384","exception":false,"start_time":"2023-03-11T21:44:13.96545","status":"completed"},"tags":[]},"source":["<a id=\"4\"></a>\n","<h1 style='background:#FF9F00;border:0; color:black;\n","    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n","    transform: rotateX(10deg);\n","    '><center style='color: #3E3D53;'>Thank you</center></h1>\n","\n","# Thank you\n","\n","**Thank you for going through this notebook**\n","\n","**If you have any suggestions please let me know**\n","\n","<a id=\"5\"></a>\n","# References \n","https://gist.github.com/karpathy/d4dee566867f8291f086\n","\n","https://arxiv.org/abs/1711.05101"]},{"cell_type":"markdown","id":"2982eefd","metadata":{"papermill":{"duration":0.014334,"end_time":"2023-03-11T21:44:14.0096","exception":false,"start_time":"2023-03-11T21:44:13.995266","status":"completed"},"tags":[]},"source":["<div style=\"padding:10px; \n","            color:#333333;\n","            margin:10px;\n","            font-size:150%;\n","            display:fill;\n","            border-radius:1px;\n","            border-style:solid;\n","            border-color:#666666;\n","            background-color:#F9F9F9;\n","            overflow:hidden;\">\n","    <center>\n","        <a id='top'></a>\n","        <b>Machine Learning From Scratch Series</b>\n","    </center>\n","    <br>\n","    <ul>\n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/linear-regression-from-scratch\" style=\"color:#0072B2\">1 - Linear Regression</a>\n","        </li>\n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/logistic-regression-from-scratch\" style=\"color:#0072B2\">2 -  Logistic Regression</a>\n","        </li>\n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/kmeans-from-scratch\" style=\"color:#0072B2\">3 - KMeans</a>\n","        </li>\n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/decision-tree-classifier-from-scratch\" style=\"color:#0072B2\">4 - Decision Trees</a>\n","        </li> \n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/random-forest-classifier-from-scratch\" style=\"color:#0072B2\">5 -  Random Forest</a>\n","        </li>\n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/knn-from-scratch\" style=\"color:#0072B2\">6 - KNearestNeighbor</a>\n","        </li>\n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/pca-from-scratch?scriptVersionId=121402593\" style=\"color:#0072B2\">7 - PCA</a>\n","        </li>\n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/svm-from-scratch\" style=\"color:#0072B2\">8 - SVM</a>\n","        </li>\n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/naive-bayes-from-scratch\" style=\"color:#0072B2\">9 - Naive Baye</a>\n","        </li>\n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/optimized-neural-network-from-scratch\" style=\"color:#0072B2\">10 - Optimized Neural Network</a>\n","        </li>\n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/neural-network-from-scratch\" style=\"color:#0072B2\">11 - Neural Network</a>\n","        </li>\n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/cnn-from-scratch\" style=\"color:#0072B2\">12 - CNN</a>\n","        </li>\n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/rnn-from-scratch\" style=\"color:#0072B2\">13 - RNN</a>\n","        </li>\n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/lstm-from-scratch\" style=\"color:#0072B2\">14 - LSTM</a>\n","        </li>\n","        <li>\n","            <a href=\"https://www.kaggle.com/code/fareselmenshawii/gru-from-scratch\" style=\"color:#0072B2\">15 - GRU</a>\n","        </li>\n","    </ul>\n","</div>"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":29125.652167,"end_time":"2023-03-11T21:44:14.728572","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-03-11T13:38:49.076405","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}