{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fareselmenshawii/linear-regression-from-scratch?scriptVersionId=144634675\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"padding:10px; \n            color:black;\n            margin:10px;\n            font-size:150%;\n            display:block;\n            border-radius:1px;\n            border-style: solid;\n            border-color:skyblue;\n            background-color:#00EFFF;\n            overflow:hidden;\">\n    <center>\n        <a id='top'></a>\n        <b style=\"color:black\">Table of Contents</b>\n    </center>\n    <ul>\n        <li>\n            <a href=\"#1\" style=\"color:black\">1 -  Overview</a>\n        </li>\n        <li>\n            <a href=\"#2\" style=\"color:black\">2 -  Imports</a>\n        </li>\n        <li>\n            <a href=\"#3\" style=\"color:black\">3 - Data Loading and Analysis</a>\n        </li>\n         <li>\n            <a href=\"#4\" style=\"color:black\">4 - Data Preprocessing</a>\n        </li>\n            <li>\n            <a href=\"#5\" style=\"color:black\">5 - Model Implementation </a>\n        </li>\n        <li>\n            <a href=\"#6\" style=\"color:black\">6 - Evaluation</a>\n        </li>\n        <li>\n            <a href=\"#7\" style=\"color:black\">7 - Thank you</a>\n        </li>\n    </ul>\n</div>\n\n\n<a id=\"1\"></a>\n<h1 style='background:#00EFFF;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Overview</center></h1>\n\n\n# Overview\n\n## Implementing Machine and Deep Learning Algorithms from Scratch\n\n**In this series of notebooks, we'll be implementing Machine and Deep Learning algorithms from scratch. We will strive to avoid using for loops and instead utilize [Vectorized Implementations](https://www.kaggle.com/code/fareselmenshawii/vectorization).**\n\n**We will primarily rely on the following libraries:**\n- **NumPy for Linear Algebra**\n- **Pandas for Data Analysis**\n- **Plotly for visualization**\n\n**The first learning algorithm we'll explore is Linear Regression.**\n**Let's get started!**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h1 style='background:#00EFFF;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Imports</center></h1>\n\n# Imports","metadata":{}},{"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:11:59.203994Z","iopub.execute_input":"2023-09-29T06:11:59.204488Z","iopub.status.idle":"2023-09-29T06:11:59.210282Z","shell.execute_reply.started":"2023-09-29T06:11:59.20445Z","shell.execute_reply":"2023-09-29T06:11:59.208899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h1 style='background:#00EFFF;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Data Loading and Analysis</center></h1>\n\n# Data Loading and Analysis","metadata":{}},{"cell_type":"code","source":"# Load the training and test datasets\ntrain_data = pd.read_csv('/kaggle/input/random-linear-regression/train.csv')\ntest_data = pd.read_csv('/kaggle/input/random-linear-regression/test.csv')\n\n# Remove rows with missing values\ntrain_data = train_data.dropna()\ntest_data = test_data.dropna()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:11:59.81915Z","iopub.execute_input":"2023-09-29T06:11:59.819667Z","iopub.status.idle":"2023-09-29T06:11:59.843495Z","shell.execute_reply.started":"2023-09-29T06:11:59.819624Z","shell.execute_reply":"2023-09-29T06:11:59.84222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:12:00.077032Z","iopub.execute_input":"2023-09-29T06:12:00.078031Z","iopub.status.idle":"2023-09-29T06:12:00.091153Z","shell.execute_reply.started":"2023-09-29T06:12:00.077975Z","shell.execute_reply":"2023-09-29T06:12:00.089876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.scatter(x=train_data['x'], y=train_data['y'],template='seaborn')","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:12:00.311569Z","iopub.execute_input":"2023-09-29T06:12:00.31257Z","iopub.status.idle":"2023-09-29T06:12:00.37534Z","shell.execute_reply.started":"2023-09-29T06:12:00.312516Z","shell.execute_reply":"2023-09-29T06:12:00.374081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The data appears to be well-suited for a Linear model, such as LinearRegression.**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h1 style='background:#00EFFF;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Data Preprocessing</center></h1>\n\n# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Set training data and target\nX_train = train_data['x'].values\ny_train = train_data['y'].values\n\n# Set testing data and target\nX_test = test_data['x'].values\ny_test = test_data['y'].values","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:12:01.005413Z","iopub.execute_input":"2023-09-29T06:12:01.005909Z","iopub.status.idle":"2023-09-29T06:12:01.014452Z","shell.execute_reply.started":"2023-09-29T06:12:01.005871Z","shell.execute_reply":"2023-09-29T06:12:01.013302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Standardize the data\n\nStandardization is a preprocessing technique used in machine learning to rescale and transform the features (variables) of a dataset to have a mean of 0 and a standard deviation of 1. It is also known as \"z-score normalization\" or \"z-score scaling.\" Standardization is an essential step in the data preprocessing pipeline for various reasons:\n\n### Why Use Standardization in Machine Learning?\n\n1. **Mean Centering**: Standardization centers the data by subtracting the mean from each feature. This ensures that the transformed data has a mean of 0. Mean centering is crucial because it helps in capturing the relative variations in the data.\n\n2. **Scale Invariance**: Standardization scales the data by dividing each feature by its standard deviation. This makes the data scale-invariant, meaning that the scale of the features no longer affects the performance of many machine learning algorithms. Without standardization, features with larger scales may dominate the learning process.\n\n3. **Improved Convergence**: Many machine learning algorithms, such as gradient-based optimization algorithms (e.g., gradient descent), converge faster when the features are standardized. It reduces the potential for numerical instability and overflow/underflow issues during training.\n\n4. **Comparability**: Standardizing the features makes it easier to compare and interpret the importance of each feature. This is especially important in models like linear regression, where the coefficients represent the feature's impact on the target variable.\n\n5. **Regularization**: In regularization techniques like Ridge and Lasso regression, the regularization strength is applied uniformly to all features. Standardization ensures that the regularization term applies fairly to all features.\n\n### How to Standardize Data\n\nThe standardization process involves the following steps:\n\n1. Calculate the mean ($\\mu$) and standard deviation ($\\sigma$) for each feature in the dataset.\n2. For each data point (sample), subtract the mean ($\\mu$) of the feature and then divide by the standard deviation ($\\sigma$) of the feature.\n\nMathematically, the standardized value for a feature `x` in a dataset is calculated as:\n\n$$\n\\text{Standardized value} = \\frac{x - \\mu}{\\sigma}\n$$\n\nHere, `x` is the original value of the feature, $\\mu$ is the mean of the feature, and $\\sigma$ is the standard deviation of the feature.\n","metadata":{}},{"cell_type":"code","source":"def standardize_data(X_train, X_test):\n    \"\"\"\n    Standardizes the input data using mean and standard deviation.\n\n    Parameters:\n        X_train (numpy.ndarray): Training data.\n        X_test (numpy.ndarray): Testing data.\n\n    Returns:\n        Tuple of standardized training and testing data.\n    \"\"\"\n    # Calculate the mean and standard deviation using the training data\n    mean = np.mean(X_train, axis=0)\n    std = np.std(X_train, axis=0)\n    \n    # Standardize the data\n    X_train = (X_train - mean) / std\n    X_test = (X_test - mean) / std\n    \n    return X_train, X_test\n\nX_train, X_test = standardize_data(X_train, X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:12:01.501765Z","iopub.execute_input":"2023-09-29T06:12:01.502632Z","iopub.status.idle":"2023-09-29T06:12:01.510504Z","shell.execute_reply.started":"2023-09-29T06:12:01.502589Z","shell.execute_reply":"2023-09-29T06:12:01.50905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reshaping data for the correct shape for the model","metadata":{}},{"cell_type":"code","source":"X_train = np.expand_dims(X_train, axis=-1)\nX_test = np.expand_dims(X_test, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:12:02.046237Z","iopub.execute_input":"2023-09-29T06:12:02.046747Z","iopub.status.idle":"2023-09-29T06:12:02.052693Z","shell.execute_reply.started":"2023-09-29T06:12:02.046706Z","shell.execute_reply":"2023-09-29T06:12:02.051389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<h1 style='background:#00EFFF;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Model Implementation</center></h1>\n\n# Model Implementation","metadata":{}},{"cell_type":"markdown","source":"# Linear Regression Model\n\nLinear regression is a fundamental model in machine learning used for predicting a continuous output variable based on input features. The model function for linear regression is represented as:\n\n$$f_{w,b}(x) = wx + b$$\n\nIn this equation, $f_{w,b}(x)$ represents the predicted output, $w$ is the weight parameter, $b$ is the bias parameter, and $x$ is the input feature.\n\n## Model Training\n\nTo train a linear regression model, we aim to find the best values for the parameters $(w, b)$ that best fit our dataset.\n\n### Forward Pass\n\nThe forward pass is a step where we compute the linear regression output for the input data $X$ using the current weights and biases. It's essentially applying our model to the input data.\n\n### Cost Function\n\nThe cost function is used to measure how well our model is performing. It quantifies the difference between the predicted values and the actual values in our dataset. The cost function is defined as:\n\n$$J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)})^2$$\n\nHere, $J(w, b)$ is the cost, $m$ is the number of training examples, $x^{(i)}$ is the input data for the $i$-th example, $y^{(i)}$ is the actual output for the $i$-th example, and $w$ and $b$ are the weight and bias parameters, respectively.\n\n### Backward Pass (Gradient Computation)\n\nThe backward pass computes the gradients of the cost function with respect to the weights and biases. These gradients are crucial for updating the model parameters during training. The gradient formulas are as follows:\n\n$$\n\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=0}^{m-1} (f_{w,b}(X^{(i)}) - y^{(i)})\n$$\n\n$$\n\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{m} \\sum_{i=0}^{m-1} (f_{w,b}(X^{(i)}) - y^{(i)})X^{(i)}\n$$\n\n## Training Process\n\nThe training process involves iteratively updating the weights and biases to minimize the cost function. This is typically done through an optimization algorithm like gradient descent. The update equations for parameters are:\n\n$$w \\leftarrow w - \\alpha \\frac{\\partial J}{\\partial w}$$\n\n$$b \\leftarrow b - \\alpha \\frac{\\partial J}{\\partial b}$$\n\nHere, $\\alpha$ represents the learning rate, which controls the step size during parameter updates.\n\nBy iteratively performing the forward pass, computing the cost, performing the backward pass, and updating the parameters, the model learns to make better predictions and fit the data.\n","metadata":{}},{"cell_type":"code","source":"class LinearRegression:\n    \"\"\"\n    Linear Regression Model with Gradient Descent\n\n    Linear regression is a supervised machine learning algorithm used for modeling the relationship\n    between a dependent variable (target) and one or more independent variables (features) by fitting\n    a linear equation to the observed data.\n\n    This class implements a linear regression model using gradient descent optimization for training.\n    It provides methods for model initialization, training, prediction, and model persistence.\n\n    Parameters:\n        learning_rate (float): The learning rate used in gradient descent.\n        convergence_tol (float, optional): The tolerance for convergence (stopping criterion). Defaults to 1e-6.\n\n    Attributes:\n        W (numpy.ndarray): Coefficients (weights) for the linear regression model.\n        b (float): Intercept (bias) for the linear regression model.\n\n    Methods:\n        initialize_parameters(n_features): Initialize model parameters.\n        forward(X): Compute the forward pass of the linear regression model.\n        compute_cost(predictions): Compute the mean squared error cost.\n        backward(predictions): Compute gradients for model parameters.\n        fit(X, y, iterations, plot_cost=True): Fit the linear regression model to training data.\n        predict(X): Predict target values for new input data.\n        save_model(filename=None): Save the trained model to a file using pickle.\n        load_model(filename): Load a trained model from a file using pickle.\n\n    Examples:\n        >>> from linear_regression import LinearRegression\n        >>> model = LinearRegression(learning_rate=0.01)\n        >>> model.fit(X_train, y_train, iterations=1000)\n        >>> predictions = model.predict(X_test)\n    \"\"\"\n\n    def __init__(self, learning_rate, convergence_tol=1e-6):\n        self.learning_rate = learning_rate\n        self.convergence_tol = convergence_tol\n        self.W = None\n        self.b = None\n\n    def initialize_parameters(self, n_features):\n        \"\"\"\n        Initialize model parameters.\n\n        Parameters:\n            n_features (int): The number of features in the input data.\n        \"\"\"\n        self.W = np.random.randn(n_features) * 0.01\n        self.b = 0\n\n    def forward(self, X):\n        \"\"\"\n        Compute the forward pass of the linear regression model.\n\n        Parameters:\n            X (numpy.ndarray): Input data of shape (m, n_features).\n\n        Returns:\n            numpy.ndarray: Predictions of shape (m,).\n        \"\"\"\n        return np.dot(X, self.W) + self.b\n\n    def compute_cost(self, predictions):\n        \"\"\"\n        Compute the mean squared error cost.\n\n        Parameters:\n            predictions (numpy.ndarray): Predictions of shape (m,).\n\n        Returns:\n            float: Mean squared error cost.\n        \"\"\"\n        m = len(predictions)\n        cost = np.sum(np.square(predictions - self.y)) / (2 * m)\n        return cost\n\n    def backward(self, predictions):\n        \"\"\"\n        Compute gradients for model parameters.\n\n        Parameters:\n            predictions (numpy.ndarray): Predictions of shape (m,).\n\n        Updates:\n            numpy.ndarray: Gradient of W.\n            float: Gradient of b.\n        \"\"\"\n        m = len(predictions)\n        self.dW = np.dot(predictions - self.y, self.X) / m\n        self.db = np.sum(predictions - self.y) / m\n\n    def fit(self, X, y, iterations, plot_cost=True):\n        \"\"\"\n        Fit the linear regression model to the training data.\n\n        Parameters:\n            X (numpy.ndarray): Training input data of shape (m, n_features).\n            y (numpy.ndarray): Training labels of shape (m,).\n            iterations (int): The number of iterations for gradient descent.\n            plot_cost (bool, optional): Whether to plot the cost during training. Defaults to True.\n\n        Raises:\n            AssertionError: If input data and labels are not NumPy arrays or have mismatched shapes.\n\n        Plots:\n            Plotly line chart showing cost vs. iteration (if plot_cost is True).\n        \"\"\"\n        assert isinstance(X, np.ndarray), \"X must be a NumPy array\"\n        assert isinstance(y, np.ndarray), \"y must be a NumPy array\"\n        assert X.shape[0] == y.shape[0], \"X and y must have the same number of samples\"\n        assert iterations > 0, \"Iterations must be greater than 0\"\n\n        self.X = X\n        self.y = y\n        self.initialize_parameters(X.shape[1])\n        costs = []\n\n        for i in range(iterations):\n            predictions = self.forward(X)\n            cost = self.compute_cost(predictions)\n            self.backward(predictions)\n            self.W -= self.learning_rate * self.dW\n            self.b -= self.learning_rate * self.db\n            costs.append(cost)\n\n            if i % 100 == 0:\n                print(f'Iteration: {i}, Cost: {cost}')\n\n            if i > 0 and abs(costs[-1] - costs[-2]) < self.convergence_tol:\n                print(f'Converged after {i} iterations.')\n                break\n\n        if plot_cost:\n            fig = px.line(y=costs, title=\"Cost vs Iteration\", template=\"plotly_dark\")\n            fig.update_layout(\n                title_font_color=\"#41BEE9\",\n                xaxis=dict(color=\"#41BEE9\", title=\"Iterations\"),\n                yaxis=dict(color=\"#41BEE9\", title=\"Cost\")\n            )\n\n            fig.show()\n\n    def predict(self, X):\n        \"\"\"\n        Predict target values for new input data.\n\n        Parameters:\n            X (numpy.ndarray): Input data of shape (m, n_features).\n\n        Returns:\n            numpy.ndarray: Predicted target values of shape (m,).\n        \"\"\"\n        return self.forward(X)\n    \n\n    def save_model(self, filename=None):\n        \"\"\"\n        Save the trained model to a file using pickle.\n\n        Parameters:\n            filename (str): The name of the file to save the model to.\n        \"\"\"\n        model_data = {\n            'learning_rate': self.learning_rate,\n            'convergence_tol': self.convergence_tol,\n            'W': self.W,\n            'b': self.b\n        }\n\n        with open(filename, 'wb') as file:\n            pickle.dump(model_data, file)\n\n    @classmethod\n    def load_model(cls, filename):\n        \"\"\"\n        Load a trained model from a file using pickle.\n\n        Parameters:\n            filename (str): The name of the file to load the model from.\n\n        Returns:\n            LinearRegression: An instance of the LinearRegression class with loaded parameters.\n        \"\"\"\n        with open(filename, 'rb') as file:\n            model_data = pickle.load(file)\n\n        # Create a new instance of the class and initialize it with the loaded parameters\n        loaded_model = cls(model_data['learning_rate'], model_data['convergence_tol'])\n        loaded_model.W = model_data['W']\n        loaded_model.b = model_data['b']\n\n        return loaded_model","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:12:02.773435Z","iopub.execute_input":"2023-09-29T06:12:02.773928Z","iopub.status.idle":"2023-09-29T06:12:02.796668Z","shell.execute_reply.started":"2023-09-29T06:12:02.773887Z","shell.execute_reply":"2023-09-29T06:12:02.795613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LinearRegression(0.01)\nlr.fit(X_train, y_train, 10000)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:12:04.222072Z","iopub.execute_input":"2023-09-29T06:12:04.223332Z","iopub.status.idle":"2023-09-29T06:12:04.309693Z","shell.execute_reply.started":"2023-09-29T06:12:04.223271Z","shell.execute_reply":"2023-09-29T06:12:04.308476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr.save_model('model.pkl')","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:12:05.415969Z","iopub.execute_input":"2023-09-29T06:12:05.417361Z","iopub.status.idle":"2023-09-29T06:12:05.424426Z","shell.execute_reply.started":"2023-09-29T06:12:05.417298Z","shell.execute_reply":"2023-09-29T06:12:05.422688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n<h1 style='background:#00EFFF;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Evaluation</center></h1>\n    \n    \n    \n# Evaluation\n\n","metadata":{}},{"cell_type":"code","source":"model = LinearRegression.load_model(\"model.pkl\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:12:06.387619Z","iopub.execute_input":"2023-09-29T06:12:06.38809Z","iopub.status.idle":"2023-09-29T06:12:06.394593Z","shell.execute_reply.started":"2023-09-29T06:12:06.388053Z","shell.execute_reply":"2023-09-29T06:12:06.393369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Mean Squared Error (MSE)\n\n**Formula:**\n$$\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true}_i} - y_{\\text{pred}_i})^2\n$$\n\n**Description:**\n- **Mean Squared Error (MSE)** is a widely used metric for evaluating the accuracy of regression models.\n- It measures the average squared difference between the predicted values ($y_{\\text{pred}}$) and the actual target values ($y_{\\text{true}}$).\n- The squared differences are averaged across all data points in the dataset.\n\n**Interpretation:**\n- A lower MSE indicates a better fit of the model to the data, as it means the model's predictions are closer to the actual values.\n- MSE is sensitive to outliers because the squared differences magnify the impact of large errors.\n\n### 2. Root Mean Squared Error (RMSE)\n\n**Formula:**\n$$\n\\text{RMSE} = \\sqrt{\\text{MSE}}\n$$\n\n**Description:**\n- **Root Mean Squared Error (RMSE)** is a variant of MSE that provides the square root of the average squared difference between predicted and actual values.\n- It is often preferred because it is in the same unit as the target variable, making it more interpretable.\n\n**Interpretation:**\n- Like MSE, a lower RMSE indicates a better fit of the model to the data.\n- RMSE is also sensitive to outliers due to the square root operation.\n\n### 3. R-squared ($R^2$)\n\n**Formula:**\n$$\nR^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}}\n$$\n\n**Description:**\n- **R-squared ($R^2$)**, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable ($y_{\\text{true}}$) that is predictable from the independent variable(s) ($y_{\\text{pred}}$) in a regression model.\n- It ranges from 0 to 1, where 0 indicates that the model does not explain any variance, and 1 indicates a perfect fit.\n\n**Interpretation:**\n- A higher $R^2$ value suggests that the model explains a larger proportion of the variance in the target variable.\n- However, $R^2$ does not provide information about the goodness of individual predictions or whether the model is overfitting or underfitting.\n","metadata":{}},{"cell_type":"code","source":"class RegressionMetrics:\n    @staticmethod\n    def mean_squared_error(y_true, y_pred):\n        \"\"\"\n        Calculate the Mean Squared Error (MSE).\n\n        Args:\n            y_true (numpy.ndarray): The true target values.\n            y_pred (numpy.ndarray): The predicted target values.\n\n        Returns:\n            float: The Mean Squared Error.\n        \"\"\"\n        assert len(y_true) == len(y_pred), \"Input arrays must have the same length.\"\n        mse = np.mean((y_true - y_pred) ** 2)\n        return mse\n\n    @staticmethod\n    def root_mean_squared_error(y_true, y_pred):\n        \"\"\"\n        Calculate the Root Mean Squared Error (RMSE).\n\n        Args:\n            y_true (numpy.ndarray): The true target values.\n            y_pred (numpy.ndarray): The predicted target values.\n\n        Returns:\n            float: The Root Mean Squared Error.\n        \"\"\"\n        assert len(y_true) == len(y_pred), \"Input arrays must have the same length.\"\n        mse = RegressionMetrics.mean_squared_error(y_true, y_pred)\n        rmse = np.sqrt(mse)\n        return rmse\n\n    @staticmethod\n    def r_squared(y_true, y_pred):\n        \"\"\"\n        Calculate the R-squared (R^2) coefficient of determination.\n\n        Args:\n            y_true (numpy.ndarray): The true target values.\n            y_pred (numpy.ndarray): The predicted target values.\n\n        Returns:\n            float: The R-squared (R^2) value.\n        \"\"\"\n        assert len(y_true) == len(y_pred), \"Input arrays must have the same length.\"\n        mean_y = np.mean(y_true)\n        ss_total = np.sum((y_true - mean_y) ** 2)\n        ss_residual = np.sum((y_true - y_pred) ** 2)\n        r2 = 1 - (ss_residual / ss_total)\n        return r2\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T05:56:51.090284Z","iopub.execute_input":"2023-09-29T05:56:51.090944Z","iopub.status.idle":"2023-09-29T05:56:51.100421Z","shell.execute_reply.started":"2023-09-29T05:56:51.090872Z","shell.execute_reply":"2023-09-29T05:56:51.099383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\nmse_value = RegressionMetrics.mean_squared_error(y_test, y_pred)\nrmse_value = RegressionMetrics.root_mean_squared_error(y_test, y_pred)\nr_squared_value = RegressionMetrics.r_squared(y_test, y_pred)\n\nprint(f\"Mean Squared Error (MSE): {mse_value}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse_value}\")\nprint(f\"R-squared (Coefficient of Determination): {r_squared_value}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T05:54:52.575962Z","iopub.execute_input":"2023-09-29T05:54:52.576446Z","iopub.status.idle":"2023-09-29T05:54:52.584259Z","shell.execute_reply.started":"2023-09-29T05:54:52.576404Z","shell.execute_reply":"2023-09-29T05:54:52.58316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n<h1 style='background:#00EFFF;border:0; color:black;\n    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n    transform: rotateX(10deg);\n    '><center style='color: #000000;'>Thank You</center></h1>\n    \n    \n    \n\n# Thank You\n\n**Thank you for going through the notebook if you haev any feedback please let me know**\n\n**Now that we know how to implement Linear Regression from scratch let's check [sklearn implementation](https://www.kaggle.com/code/fareselmenshawii/linear-regression)**\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"padding:10px; \n            color:#333333;\n            margin:10px;\n            font-size:150%;\n            display:fill;\n            border-radius:1px;\n            border-style:solid;\n            border-color:#666666;\n            background-color:#F9F9F9;\n            overflow:hidden;\">\n    <center>\n        <a id='top'></a>\n        <b>Machine Learning From Scratch Series</b>\n    </center>\n    <br>\n    <ul>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/linear-regression-from-scratch\" style=\"color:#0072B2\">1 - Linear Regression</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/logistic-regression-from-scratch\" style=\"color:#0072B2\">2 -  Logistic Regression</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/kmeans-from-scratch\" style=\"color:#0072B2\">3 - KMeans</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/decision-tree-classifier-from-scratch\" style=\"color:#0072B2\">4 - Decision Trees</a>\n        </li> \n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/random-forest-classifier-from-scratch\" style=\"color:#0072B2\">5 -  Random Forest</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/knn-from-scratch\" style=\"color:#0072B2\">6 - KNearestNeighbor</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/pca-from-scratch?scriptVersionId=121402593\" style=\"color:#0072B2\">7 - PCA</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/svm-from-scratch\" style=\"color:#0072B2\">8 - SVM</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/naive-bayes-from-scratch\" style=\"color:#0072B2\">9 - Naive Baye</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/optimized-neural-network-from-scratch\" style=\"color:#0072B2\">10 - Optimized Neural Network</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/neural-network-from-scratch\" style=\"color:#0072B2\">11 - Neural Network</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/cnn-from-scratch\" style=\"color:#0072B2\">12 - CNN</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/rnn-from-scratch\" style=\"color:#0072B2\">13 - RNN</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/lstm-from-scratch\" style=\"color:#0072B2\">14 - LSTM</a>\n        </li>\n        <li>\n            <a href=\"https://www.kaggle.com/code/fareselmenshawii/gru-from-scratch\" style=\"color:#0072B2\">15 - GRU</a>\n        </li>\n    </ul>\n</div>","metadata":{}}]}